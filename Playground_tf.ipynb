{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from bounding_box_utils.bounding_box_utils import iou, convert_coordinates\n",
    "from ssd_encoder_decoder.matching_utils import match_bipartite_greedy, match_multi\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = np.load(\"outputs/predder.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt1 = gt[1]['predictions_1']\n",
    "gt2 = gt[1]['predictions_2']\n",
    "gt1_proj = gt[1]['predictions_1_proj_tot']\n",
    "gt2_proj = gt[1]['predictions_2_proj_tot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = np.load(\"outputs/predictions_1_14.npy\")\n",
    "pred_1_proj = np.load(\"outputs/predictions_1_proj_14.npy\")\n",
    "pred_2 = np.load(\"outputs/predictions_2_14.npy\")\n",
    "pred_2_proj = np.load(\"outputs/predictions_2_proj_14.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels:  (5, 4)\n",
    "y_encoded:  (17292, 4)\n",
    "similarities:  (5, 17292)\n",
    "bipartite_matches:  (5,)\n",
    "similarities after bipartite:  (5, 17292)\n",
    "matches:  (53,)  -  (53,)\n",
    "similarities after matches:  (5, 17292)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_1 = gt1_proj[:,:,:18]\n",
    "y_pred_1 = pred_1_proj[:,:,18:]\n",
    "y_true_2 = gt1_proj[:,:,18:]\n",
    "y_pred_2 = pred_1_proj[:,:,18:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = tf.shape(y_pred_1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_1d = tf.convert_to_tensor(np.concatenate([y_true_1,y_true_1]),dtype=tf.float32)\n",
    "y_pred_1d = tf.convert_to_tensor(np.concatenate([y_pred_1,y_pred_1]),dtype=tf.float32)\n",
    "y_true_2d = tf.convert_to_tensor(np.concatenate([y_true_2,y_true_2]),dtype=tf.float32)\n",
    "y_pred_2d = tf.convert_to_tensor(np.concatenate([y_pred_2,y_pred_2]),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_27:0' shape=(2, 17292, 18) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_1d[:,:,:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_ratio = 3\n",
    "n_neg_min = 0\n",
    "alpha = 1.0\n",
    "\n",
    "def smooth_L1_loss(y_true, y_pred):\n",
    "    '''\n",
    "    Compute smooth L1 loss, see references.\n",
    "\n",
    "    Arguments:\n",
    "        y_true (nD tensor): A TensorFlow tensor of any shape containing the ground truth data.\n",
    "            In this context, the expected tensor has shape `(batch_size, #boxes, 4)` and\n",
    "            contains the ground truth bounding box coordinates, where the last dimension\n",
    "            contains `(xmin, xmax, ymin, ymax)`.\n",
    "        y_pred (nD tensor): A TensorFlow tensor of identical structure to `y_true` containing\n",
    "            the predicted data, in this context the predicted bounding box coordinates.\n",
    "\n",
    "    Returns:\n",
    "        The smooth L1 loss, a nD-1 Tensorflow tensor. In this context a 2D tensor\n",
    "        of shape (batch, n_boxes_total).\n",
    "\n",
    "    References:\n",
    "        https://arxiv.org/abs/1504.08083\n",
    "    '''\n",
    "    absolute_loss = tf.abs(y_true - y_pred)\n",
    "    square_loss = 0.5 * (y_true - y_pred)**2\n",
    "    l1_loss = tf.where(tf.less(absolute_loss, 1.0), square_loss, absolute_loss - 0.5)\n",
    "    return tf.reduce_sum(l1_loss, axis=-1)\n",
    "\n",
    "def log_loss(y_true, y_pred):\n",
    "    '''\n",
    "    Compute the softmax log loss.\n",
    "\n",
    "    Arguments:\n",
    "        y_true (nD tensor): A TensorFlow tensor of any shape containing the ground truth data.\n",
    "            In this context, the expected tensor has shape (batch_size, #boxes, #classes)\n",
    "            and contains the ground truth bounding box categories.\n",
    "        y_pred (nD tensor): A TensorFlow tensor of identical structure to `y_true` containing\n",
    "            the predicted data, in this context the predicted bounding box categories.\n",
    "\n",
    "    Returns:\n",
    "        The softmax log loss, a nD-1 Tensorflow tensor. In this context a 2D tensor\n",
    "        of shape (batch, n_boxes_total).\n",
    "    '''\n",
    "    # Make sure that `y_pred` doesn't contain any zeros (which would break the log function)\n",
    "    y_pred = tf.maximum(y_pred, 1e-15)\n",
    "    # Compute the log loss\n",
    "    log_loss = -tf.reduce_sum(y_true * tf.log(y_pred), axis=-1)\n",
    "    return log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "def gt_rem(pred, gt):\n",
    "    val = tf.subtract(tf.shape(pred)[1], tf.shape(gt)[1],name=\"gt_rem_subtract\")\n",
    "    gt = tf.slice(gt, [0, 0, 0], [1, tf.shape(pred)[1], 18],name=\"rem_slice\")\n",
    "    return gt\n",
    "\n",
    "def gt_add(pred, gt):\n",
    "    #add to gt\n",
    "    val = tf.subtract(tf.shape(pred)[1], tf.shape(gt)[1],name=\"gt_add_subtract\")\n",
    "    ext = tf.slice(gt, [0, 0, 0], [1, val, 18], name=\"add_slice\")\n",
    "    gt = K.concatenate([ext,gt], axis=1)\n",
    "    return gt\n",
    "\n",
    "def equalalready(gt, pred): return pred\n",
    "\n",
    "def make_equal(pred, gt):\n",
    "    equal_tensor = tf.cond(tf.shape(pred)[1] < tf.shape(gt)[1], lambda: gt_rem(pred, gt), lambda: gt_add(pred, gt), name=\"make_equal_cond\")\n",
    "    return equal_tensor\n",
    "\n",
    "def matcher(y_true_1,y_true_2,y_pred_1,y_pred_2, bsz):\n",
    "    pred = 0\n",
    "    gt = 0\n",
    "    for i in range(bsz):\n",
    "        filterer = tf.where(tf.not_equal(y_true_1[i,:,-1],99))\n",
    "        y_true_new = tf.gather_nd(y_true_1[i,:,:],filterer)\n",
    "        y_true_new = tf.expand_dims(y_true_new, 0)\n",
    "        iou_out = tf.py_func(iou, [y_true_new[0,:,-16:-12],tf.convert_to_tensor(y_true_1[i,:,-16:-12])], tf.float64, name=\"iou_out\")\n",
    "        bipartite_matches = tf.py_func(match_bipartite_greedy, [iou_out], tf.int64, name=\"bipartite_matches\")\n",
    "        out = tf.gather(y_pred_2[i,:,:], [bipartite_matches], axis=0, name=\"out\")\n",
    "        \n",
    "        filterer_2 = tf.where(tf.not_equal(y_true_2[i,:,-1],99))\n",
    "        y_true_2_new = tf.gather_nd(y_true_2[i,:,:],filterer_2)\n",
    "        y_true_2_new = tf.expand_dims(y_true_2_new, 0)\n",
    "\n",
    "        box_comparer = tf.reduce_all(tf.equal(tf.shape(out)[1], tf.shape(y_true_2_new)[1]), name=\"box_comparer\")\n",
    "        y_true_2_equal = tf.cond(box_comparer, lambda: equalalready(out, y_true_2_new), lambda: make_equal(out, y_true_2_new), name=\"y_true_cond\")\n",
    "        if i != 0:\n",
    "            pred = K.concatenate([pred,out], axis=-1)\n",
    "            gt = K.concatenate([gt,y_true_2_equal], axis=0)\n",
    "        else:\n",
    "            pred = out\n",
    "            gt = y_true_2_equal\n",
    "    return pred, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = matcher(y_true_1d,y_true_2d,y_pred_1d,y_pred_2d,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_loss:  (2, 4)\n",
      "localization_loss:  (2, 4)\n"
     ]
    }
   ],
   "source": [
    "batch_size = tf.shape(y_pred)[0] # Output dtype: tf.int32\n",
    "n_boxes = tf.shape(y_pred)[1] # Output dtype: tf.int32, note that `n_boxes` in this context denotes the total number of boxes per image, not the number of boxes per cell.\n",
    "\n",
    "# 1: Compute the losses for class and box predictions for every box.\n",
    "\n",
    "classification_loss = tf.to_float(log_loss(y_true[:,:,:-16], y_pred[:,:,:-16])) # Output shape: (batch_size, n_boxes)\n",
    "localization_loss = tf.to_float(smooth_L1_loss(y_true[:,:,-16:-12], y_pred[:,:,-16:-12])) # Output shape: (batch_size, n_boxes)\n",
    "\n",
    "print(\"classification_loss: \",K.eval(classification_loss).shape)\n",
    "print(\"localization_loss: \",K.eval(localization_loss).shape)\n",
    "\n",
    "negatives = y_true[:,:,0] # Tensor of shape (batch_size, n_boxes)\n",
    "positives = tf.to_float(tf.reduce_max(y_true[:,:,1:-16], axis=-1)) # Tensor of shape (batch_size, n_boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_positive:  ()\n",
      "pos_class_loss:  (2,)\n",
      "neg_class_loss_all:  (2, 4)\n",
      "n_neg_losses:  ()\n"
     ]
    }
   ],
   "source": [
    "n_positive = tf.reduce_sum(positives)\n",
    "pos_class_loss = tf.reduce_sum(classification_loss * positives, axis=-1) # Tensor of shape (batch_size,)\n",
    "neg_class_loss_all = classification_loss * negatives # Tensor of shape (batch_size, n_boxes)\n",
    "n_neg_losses = tf.count_nonzero(neg_class_loss_all, dtype=tf.int32) # The number of non-zero loss entries in `neg_class_loss_all`\n",
    "print(\"n_positive: \",K.eval(n_positive).shape)\n",
    "print(\"pos_class_loss: \",K.eval(pos_class_loss).shape)\n",
    "print(\"neg_class_loss_all: \",K.eval(neg_class_loss_all).shape)\n",
    "print(\"n_neg_losses: \",K.eval(n_neg_losses).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_negative_keep:  ()\n",
      "neg_class_loss:  (2,)\n",
      "class_loss:  (2,)\n",
      "loc_loss:  (2,)\n",
      "total_loss:  (2,)\n",
      "total_loss:  (2,)\n"
     ]
    }
   ],
   "source": [
    "n_negative_keep = tf.minimum(tf.maximum(neg_pos_ratio * tf.to_int32(n_positive), n_neg_min), n_neg_losses)\n",
    "print(\"n_negative_keep: \",K.eval(n_negative_keep).shape)\n",
    "\n",
    "# In the unlikely case when either (1) there are no negative ground truth boxes at all\n",
    "# or (2) the classification loss for all negative boxes is zero, return zero as the `neg_class_loss`.\n",
    "def f1():\n",
    "    return tf.zeros([batch_size])\n",
    "# Otherwise compute the negative loss.\n",
    "def f2():\n",
    "    # Now we'll identify the top-k (where k == `n_negative_keep`) boxes with the highest confidence loss that\n",
    "    # belong to the background class in the ground truth data. Note that this doesn't necessarily mean that the model\n",
    "    # predicted the wrong class for those boxes, it just means that the loss for those boxes is the highest.\n",
    "\n",
    "    # To do this, we reshape `neg_class_loss_all` to 1D...\n",
    "    neg_class_loss_all_1D = tf.reshape(neg_class_loss_all, [-1]) # Tensor of shape (batch_size * n_boxes,)\n",
    "    # ...and then we get the indices for the `n_negative_keep` boxes with the highest loss out of those...\n",
    "    values, indices = tf.nn.top_k(neg_class_loss_all_1D,\n",
    "            y_true[:,:,:18]                      k=n_negative_keep,\n",
    "                                  sorted=False) # We don't need them sorted.\n",
    "    # ...and with these indices we'll create a mask...\n",
    "    negatives_keep = tf.scatter_nd(indices=tf.expand_dims(indices, axis=1),\n",
    "                                   updates=tf.ones_like(indices, dtype=tf.int32),\n",
    "                                   shape=tf.shape(neg_class_loss_all_1D)) # Tensor of shape (batch_size * n_boxes,)\n",
    "    negatives_keep = tf.to_float(tf.reshape(negatives_keep, [batch_size, n_boxes])) # Tensor of shape (batch_size, n_boxes)\n",
    "    # ...and use it to keep only those boxes and mask all other classification losses\n",
    "    neg_class_loss = tf.reduce_sum(classification_loss * negatives_keep, axis=-1) # Tensor of shape (batch_size,)\n",
    "    return neg_class_loss\n",
    "\n",
    "neg_class_loss = tf.cond(tf.equal(n_neg_losses, tf.constant(0)), f1, f2)\n",
    "print(\"neg_class_loss: \",K.eval(neg_class_loss).shape)\n",
    "\n",
    "class_loss = pos_class_loss + neg_class_loss # Tensor of shape (batch_size,)\n",
    "print(\"class_loss: \",K.eval(neg_class_loss).shape)\n",
    "\n",
    "# 3: Compute the localization loss for the positive targets.\n",
    "#    We don't compute a localization loss for negative predicted boxes (obviously: there are no ground truth boxes they would correspond to).\n",
    "\n",
    "loc_loss = tf.reduce_sum(localization_loss * positives, axis=-1) # Tensor of shape (batch_size,)\n",
    "print(\"loc_loss: \",K.eval(loc_loss).shape)\n",
    "\n",
    "# 4: Compute the total loss.\n",
    "\n",
    "total_loss = (class_loss + alpha * loc_loss) / tf.maximum(1.0, n_positive) # In case `n_positive == 0`\n",
    "# Keras has the annoying habit of dividing the loss by the batch size, which sucks in our case\n",
    "# because the relevant criterion to average our loss over is the number of positive boxes in the batch\n",
    "# (by which we're dividing in the line above), not the batch size. So in order to revert Keras' averaging\n",
    "# over the batch size, we'll have to multiply by it.\n",
    "print(\"total_loss: \",K.eval(total_loss).shape)\n",
    "\n",
    "total_loss = total_loss * tf.to_float(batch_size)\n",
    "print(\"total_loss: \",K.eval(total_loss).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_loss.set_shape((None,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.552272, 14.552272], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.eval(total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.constant([[[1, 2, 3], [3, 4, 5]],\n",
    "                  [[3, 3, 3], [4, 4, 4]],\n",
    "                  [[5, 5, 5], [6, 6, 6]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 3],\n",
       "        [3, 4, 5]],\n",
       "\n",
       "       [[3, 3, 3],\n",
       "        [4, 4, 4]],\n",
       "\n",
       "       [[5, 5, 5],\n",
       "        [6, 6, 6]]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.eval(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2],\n",
       "        [3, 4]],\n",
       "\n",
       "       [[3, 3],\n",
       "        [4, 4]],\n",
       "\n",
       "       [[5, 5],\n",
       "        [6, 6]]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.eval(tf.slice(t, [0, 0, 0], [-1, -1, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3],\n",
       "        [5]],\n",
       "\n",
       "       [[3],\n",
       "        [4]],\n",
       "\n",
       "       [[5],\n",
       "        [6]]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.eval(tf.slice(t, [0, 0, 7], [-1, -1, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
