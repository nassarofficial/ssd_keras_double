{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD300 Training Tutorial\n",
    "\n",
    "This tutorial explains how to train an SSD300 on the Pascal VOC datasets. The preset parameters reproduce the training of the original SSD300 \"07+12\" model. Training SSD512 works simiarly, so there's no extra tutorial for that. The same goes for training on other datasets.\n",
    "\n",
    "You can find a summary of a full training here to get an impression of what it should look like:\n",
    "[SSD300 \"07+12\" training summary](https://github.com/pierluigiferrari/ssd_keras/blob/master/training_summaries/ssd300_pascal_07%2B12_training_summary.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger, EarlyStopping, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Model\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "from imageio import imread\n",
    "\n",
    "from models.keras_ssd300 import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss_mod import SSDLoss\n",
    "from keras_loss_function.keras_ssd_loss_proj import SSDLoss_proj\n",
    "\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder_mod import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize_Modified\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels_Modified\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation_modified\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "from bounding_box_utils.bounding_box_utils import iou, convert_coordinates\n",
    "from ssd_encoder_decoder.matching_utils import match_bipartite_greedy, match_multi\n",
    "import random\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "import tensorflow as tf\n",
    "np.random.seed(1337)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preliminary note\n",
    "\n",
    "All places in the code where you need to make any changes are marked `TODO` and explained accordingly. All code cells that don't contain `TODO` markers just need to be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set the model configuration parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 300 # Height of the model input images\n",
    "img_width = 600 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
    "n_classes = 1 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales = scales_pascal\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True            # print(y_encoded)\n",
    "\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "normalize_coords = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build or load the model\n",
    "\n",
    "You will want to execute either of the two code cells in the subsequent two sub-sections, not both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nassar/gits/ssd_keras_double/models/keras_ssd300.py:612: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "  model = Model(input=[x,geo_1,geo_2],output=[predictions, predictions_proj])\n"
     ]
    }
   ],
   "source": [
    "# 1: Build the Keras model.\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                mode='training',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                clip_boxes=clip_boxes,\n",
    "                variances=variances,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=mean_color,\n",
    "                swap_channels=swap_channels)\n",
    "\n",
    "# 2: Load some weights into the model.\n",
    "\n",
    "# TODO: Set the path to the weights you want to load.\n",
    "weights_path = 'weights/VGG_ILSVRC_16_layers_fc_reduced.h5'\n",
    "\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "# 3: Instantiate an optimizer and the SSD loss function and compile the model.\n",
    "#    If you want to follow the original Caffe implementation, use the preset SGD\n",
    "#    optimizer, otherwise I'd recommend the commented-out Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 300, 600, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 300, 600, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "identity_layer__1 (Lambda)       (None, 300, 600, 3)   0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "identity_layer__2 (Lambda)       (None, 300, 600, 3)   0           input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "input_mean_normalization__1 (Lam (None, 300, 600, 3)   0           identity_layer__1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "input_mean_normalization__2 (Lam (None, 300, 600, 3)   0           identity_layer__2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "input_channel_swap__1 (Lambda)   (None, 300, 600, 3)   0           input_mean_normalization__1[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "input_channel_swap__2 (Lambda)   (None, 300, 600, 3)   0           input_mean_normalization__2[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "conv1_1__1 (Conv2D)              (None, 300, 600, 64)  1792        input_channel_swap__1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1_1__2 (Conv2D)              (None, 300, 600, 64)  1792        input_channel_swap__2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1_2__1 (Conv2D)              (None, 300, 600, 64)  36928       conv1_1__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv1_2__2 (Conv2D)              (None, 300, 600, 64)  36928       conv1_1__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool1__1 (MaxPooling2D)          (None, 150, 300, 64)  0           conv1_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool1__2 (MaxPooling2D)          (None, 150, 300, 64)  0           conv1_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_1__1 (Conv2D)              (None, 150, 300, 128) 73856       pool1__1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2_1__2 (Conv2D)              (None, 150, 300, 128) 73856       pool1__2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2_2__1 (Conv2D)              (None, 150, 300, 128) 147584      conv2_1__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_2__2 (Conv2D)              (None, 150, 300, 128) 147584      conv2_1__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool2__1 (MaxPooling2D)          (None, 75, 150, 128)  0           conv2_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool2__2 (MaxPooling2D)          (None, 75, 150, 128)  0           conv2_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_1__1 (Conv2D)              (None, 75, 150, 256)  295168      pool2__1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv3_1__2 (Conv2D)              (None, 75, 150, 256)  295168      pool2__2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv3_2__1 (Conv2D)              (None, 75, 150, 256)  590080      conv3_1__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_2__2 (Conv2D)              (None, 75, 150, 256)  590080      conv3_1__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_3__1 (Conv2D)              (None, 75, 150, 256)  590080      conv3_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_3__2 (Conv2D)              (None, 75, 150, 256)  590080      conv3_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool3__1 (MaxPooling2D)          (None, 38, 75, 256)   0           conv3_3__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool3__2 (MaxPooling2D)          (None, 38, 75, 256)   0           conv3_3__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_1__1 (Conv2D)              (None, 38, 75, 512)   1180160     pool3__1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv4_1__2 (Conv2D)              (None, 38, 75, 512)   1180160     pool3__2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv4_2__1 (Conv2D)              (None, 38, 75, 512)   2359808     conv4_1__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_2__2 (Conv2D)              (None, 38, 75, 512)   2359808     conv4_1__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3__1 (Conv2D)              (None, 38, 75, 512)   2359808     conv4_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3__2 (Conv2D)              (None, 38, 75, 512)   2359808     conv4_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool4__1 (MaxPooling2D)          (None, 19, 38, 512)   0           conv4_3__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool4__2 (MaxPooling2D)          (None, 19, 38, 512)   0           conv4_3__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_1__1 (Conv2D)              (None, 19, 38, 512)   2359808     pool4__1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv5_1__2 (Conv2D)              (None, 19, 38, 512)   2359808     pool4__2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv5_2__1 (Conv2D)              (None, 19, 38, 512)   2359808     conv5_1__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_2__2 (Conv2D)              (None, 19, 38, 512)   2359808     conv5_1__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_3__1 (Conv2D)              (None, 19, 38, 512)   2359808     conv5_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_3__2 (Conv2D)              (None, 19, 38, 512)   2359808     conv5_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool5__1 (MaxPooling2D)          (None, 19, 38, 512)   0           conv5_3__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool5__2 (MaxPooling2D)          (None, 19, 38, 512)   0           conv5_3__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "fc6__1 (Conv2D)                  (None, 19, 38, 1024)  4719616     pool5__1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "fc6__2 (Conv2D)                  (None, 19, 38, 1024)  4719616     pool5__2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "fc7__1 (Conv2D)                  (None, 19, 38, 1024)  1049600     fc6__1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "fc7__2 (Conv2D)                  (None, 19, 38, 1024)  1049600     fc6__2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv6_1__1 (Conv2D)              (None, 19, 38, 256)   262400      fc7__1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv6_1__2 (Conv2D)              (None, 19, 38, 256)   262400      fc7__2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv6adding__1 (ZeroPadding2D)   (None, 21, 40, 256)   0           conv6_1__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv6adding__2 (ZeroPadding2D)   (None, 21, 40, 256)   0           conv6_1__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2__1 (Conv2D)              (None, 10, 19, 512)   1180160     conv6adding__1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2__2 (Conv2D)              (None, 10, 19, 512)   1180160     conv6adding__2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv7_1__1 (Conv2D)              (None, 10, 19, 128)   65664       conv6_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv7_1__2 (Conv2D)              (None, 10, 19, 128)   65664       conv6_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv7adding__1 (ZeroPadding2D)   (None, 12, 21, 128)   0           conv7_1__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv7adding__2 (ZeroPadding2D)   (None, 12, 21, 128)   0           conv7_1__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2__1 (Conv2D)              (None, 5, 10, 256)    295168      conv7adding__1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2__2 (Conv2D)              (None, 5, 10, 256)    295168      conv7adding__2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv8_1__1 (Conv2D)              (None, 5, 10, 128)    32896       conv7_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv8_1__2 (Conv2D)              (None, 5, 10, 128)    32896       conv7_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2__1 (Conv2D)              (None, 3, 8, 256)     295168      conv8_1__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2__2 (Conv2D)              (None, 3, 8, 256)     295168      conv8_1__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv9_1__1 (Conv2D)              (None, 3, 8, 128)     32896       conv8_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv9_1__2 (Conv2D)              (None, 3, 8, 128)     32896       conv8_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm__1 (L2Normalization (None, 38, 75, 512)   512         conv4_3__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2__1 (Conv2D)              (None, 1, 6, 256)     295168      conv9_1__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm__2 (L2Normalization (None, 38, 75, 512)   512         conv4_3__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2__2 (Conv2D)              (None, 1, 6, 256)     295168      conv9_1__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_loc__1 (Conv2D (None, 38, 75, 16)    73744       conv4_3_norm__1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_loc__1 (Conv2D)         (None, 19, 38, 24)    221208      fc7__1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_loc__1 (Conv2D)     (None, 10, 19, 24)    110616      conv6_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_loc__1 (Conv2D)     (None, 5, 10, 24)     55320       conv7_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_loc__1 (Conv2D)     (None, 3, 8, 16)      36880       conv8_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_loc__1 (Conv2D)     (None, 1, 6, 16)      36880       conv9_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_loc__2 (Conv2D (None, 38, 75, 16)    73744       conv4_3_norm__2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_loc__2 (Conv2D)         (None, 19, 38, 24)    221208      fc7__2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_loc__2 (Conv2D)     (None, 10, 19, 24)    110616      conv6_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_loc__2 (Conv2D)     (None, 5, 10, 24)     55320       conv7_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_loc__2 (Conv2D)     (None, 3, 8, 16)      36880       conv8_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_loc__2 (Conv2D)     (None, 1, 6, 16)      36880       conv9_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_conf__1 (Conv2 (None, 38, 75, 8)     36872       conv4_3_norm__1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_conf__1 (Conv2D)        (None, 19, 38, 12)    110604      fc7__1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_conf__1 (Conv2D)    (None, 10, 19, 12)    55308       conv6_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_conf__1 (Conv2D)    (None, 5, 10, 12)     27660       conv7_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_conf__1 (Conv2D)    (None, 3, 8, 8)       18440       conv8_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_conf__1 (Conv2D)    (None, 1, 6, 8)       18440       conv9_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_priorbox__1 (A (None, 38, 75, 4, 8)  0           conv4_3_norm_mbox_loc__1[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_priorbox__1 (AnchorBoxe (None, 19, 38, 6, 8)  0           fc7_mbox_loc__1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_priorbox__1 (Anchor (None, 10, 19, 6, 8)  0           conv6_2_mbox_loc__1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_priorbox__1 (Anchor (None, 5, 10, 6, 8)   0           conv7_2_mbox_loc__1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_priorbox__1 (Anchor (None, 3, 8, 4, 8)    0           conv8_2_mbox_loc__1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_priorbox__1 (Anchor (None, 1, 6, 4, 8)    0           conv9_2_mbox_loc__1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_conf__2 (Conv2 (None, 38, 75, 8)     36872       conv4_3_norm__2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_conf__2 (Conv2D)        (None, 19, 38, 12)    110604      fc7__2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_conf__2 (Conv2D)    (None, 10, 19, 12)    55308       conv6_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_conf__2 (Conv2D)    (None, 5, 10, 12)     27660       conv7_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_conf__2 (Conv2D)    (None, 3, 8, 8)       18440       conv8_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_conf__2 (Conv2D)    (None, 1, 6, 8)       18440       conv9_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_priorbox__2 (A (None, 38, 75, 4, 8)  0           conv4_3_norm_mbox_loc__2[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_priorbox__2 (AnchorBoxe (None, 19, 38, 6, 8)  0           fc7_mbox_loc__2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_priorbox__2 (Anchor (None, 10, 19, 6, 8)  0           conv6_2_mbox_loc__2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_priorbox__2 (Anchor (None, 5, 10, 6, 8)   0           conv7_2_mbox_loc__2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_priorbox__2 (Anchor (None, 3, 8, 4, 8)    0           conv8_2_mbox_loc__2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_priorbox__2 (Anchor (None, 1, 6, 4, 8)    0           conv9_2_mbox_loc__2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_conf_reshape__ (None, 11400, 2)      0           conv4_3_norm_mbox_conf__1[0][0]  \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_conf_reshape__1 (Reshap (None, 4332, 2)       0           fc7_mbox_conf__1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_conf_reshape__1 (Re (None, 1140, 2)       0           conv6_2_mbox_conf__1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_conf_reshape__1 (Re (None, 300, 2)        0           conv7_2_mbox_conf__1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_conf_reshape__1 (Re (None, 96, 2)         0           conv8_2_mbox_conf__1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_conf_reshape__1 (Re (None, 24, 2)         0           conv9_2_mbox_conf__1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_loc_reshape__1 (None, 11400, 4)      0           conv4_3_norm_mbox_loc__1[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_loc_reshape__1 (Reshape (None, 4332, 4)       0           fc7_mbox_loc__1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_loc_reshape__1 (Res (None, 1140, 4)       0           conv6_2_mbox_loc__1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_loc_reshape__1 (Res (None, 300, 4)        0           conv7_2_mbox_loc__1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_loc_reshape__1 (Res (None, 96, 4)         0           conv8_2_mbox_loc__1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_loc_reshape__1 (Res (None, 24, 4)         0           conv9_2_mbox_loc__1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_priorbox_resha (None, 11400, 8)      0           conv4_3_norm_mbox_priorbox__1[0][\n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_priorbox_reshape__1 (Re (None, 4332, 8)       0           fc7_mbox_priorbox__1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_priorbox_reshape__1 (None, 1140, 8)       0           conv6_2_mbox_priorbox__1[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_priorbox_reshape__1 (None, 300, 8)        0           conv7_2_mbox_priorbox__1[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_priorbox_reshape__1 (None, 96, 8)         0           conv8_2_mbox_priorbox__1[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_priorbox_reshape__1 (None, 24, 8)         0           conv9_2_mbox_priorbox__1[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_conf_reshape__ (None, 11400, 2)      0           conv4_3_norm_mbox_conf__2[0][0]  \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_conf_reshape__2 (Reshap (None, 4332, 2)       0           fc7_mbox_conf__2[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_conf_reshape__2 (Re (None, 1140, 2)       0           conv6_2_mbox_conf__2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_conf_reshape__2 (Re (None, 300, 2)        0           conv7_2_mbox_conf__2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_conf_reshape__2 (Re (None, 96, 2)         0           conv8_2_mbox_conf__2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_conf_reshape__2 (Re (None, 24, 2)         0           conv9_2_mbox_conf__2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_loc_reshape__2 (None, 11400, 4)      0           conv4_3_norm_mbox_loc__2[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_loc_reshape__2 (Reshape (None, 4332, 4)       0           fc7_mbox_loc__2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_loc_reshape__2 (Res (None, 1140, 4)       0           conv6_2_mbox_loc__2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_loc_reshape__2 (Res (None, 300, 4)        0           conv7_2_mbox_loc__2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_loc_reshape__2 (Res (None, 96, 4)         0           conv8_2_mbox_loc__2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_loc_reshape__2 (Res (None, 24, 4)         0           conv9_2_mbox_loc__2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_priorbox_resha (None, 11400, 8)      0           conv4_3_norm_mbox_priorbox__2[0][\n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_priorbox_reshape__2 (Re (None, 4332, 8)       0           fc7_mbox_priorbox__2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_priorbox_reshape__2 (None, 1140, 8)       0           conv6_2_mbox_priorbox__2[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_priorbox_reshape__2 (None, 300, 8)        0           conv7_2_mbox_priorbox__2[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_priorbox_reshape__2 (None, 96, 8)         0           conv8_2_mbox_priorbox__2[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_priorbox_reshape__2 (None, 24, 8)         0           conv9_2_mbox_priorbox__2[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "mbox_conf__1 (Concatenate)       (None, 17292, 2)      0           conv4_3_norm_mbox_conf_reshape__1\n",
      "                                                                   fc7_mbox_conf_reshape__1[0][0]   \n",
      "                                                                   conv6_2_mbox_conf_reshape__1[0][0\n",
      "                                                                   conv7_2_mbox_conf_reshape__1[0][0\n",
      "                                                                   conv8_2_mbox_conf_reshape__1[0][0\n",
      "                                                                   conv9_2_mbox_conf_reshape__1[0][0\n",
      "____________________________________________________________________________________________________\n",
      "mbox_loc__1 (Concatenate)        (None, 17292, 4)      0           conv4_3_norm_mbox_loc_reshape__1[\n",
      "                                                                   fc7_mbox_loc_reshape__1[0][0]    \n",
      "                                                                   conv6_2_mbox_loc_reshape__1[0][0]\n",
      "                                                                   conv7_2_mbox_loc_reshape__1[0][0]\n",
      "                                                                   conv8_2_mbox_loc_reshape__1[0][0]\n",
      "                                                                   conv9_2_mbox_loc_reshape__1[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "mbox_priorbox__1 (Concatenate)   (None, 17292, 8)      0           conv4_3_norm_mbox_priorbox_reshap\n",
      "                                                                   fc7_mbox_priorbox_reshape__1[0][0\n",
      "                                                                   conv6_2_mbox_priorbox_reshape__1[\n",
      "                                                                   conv7_2_mbox_priorbox_reshape__1[\n",
      "                                                                   conv8_2_mbox_priorbox_reshape__1[\n",
      "                                                                   conv9_2_mbox_priorbox_reshape__1[\n",
      "____________________________________________________________________________________________________\n",
      "mbox_conf__2 (Concatenate)       (None, 17292, 2)      0           conv4_3_norm_mbox_conf_reshape__2\n",
      "                                                                   fc7_mbox_conf_reshape__2[0][0]   \n",
      "                                                                   conv6_2_mbox_conf_reshape__2[0][0\n",
      "                                                                   conv7_2_mbox_conf_reshape__2[0][0\n",
      "                                                                   conv8_2_mbox_conf_reshape__2[0][0\n",
      "                                                                   conv9_2_mbox_conf_reshape__2[0][0\n",
      "____________________________________________________________________________________________________\n",
      "mbox_loc__2 (Concatenate)        (None, 17292, 4)      0           conv4_3_norm_mbox_loc_reshape__2[\n",
      "                                                                   fc7_mbox_loc_reshape__2[0][0]    \n",
      "                                                                   conv6_2_mbox_loc_reshape__2[0][0]\n",
      "                                                                   conv7_2_mbox_loc_reshape__2[0][0]\n",
      "                                                                   conv8_2_mbox_loc_reshape__2[0][0]\n",
      "                                                                   conv9_2_mbox_loc_reshape__2[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "mbox_priorbox__2 (Concatenate)   (None, 17292, 8)      0           conv4_3_norm_mbox_priorbox_reshap\n",
      "                                                                   fc7_mbox_priorbox_reshape__2[0][0\n",
      "                                                                   conv6_2_mbox_priorbox_reshape__2[\n",
      "                                                                   conv7_2_mbox_priorbox_reshape__2[\n",
      "                                                                   conv8_2_mbox_priorbox_reshape__2[\n",
      "                                                                   conv9_2_mbox_priorbox_reshape__2[\n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 17292, 3)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 17292, 3)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "mbox_conf_softmax__1 (Activation (None, 17292, 2)      0           mbox_conf__1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 17292, 4)      0           mbox_loc__1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "mbox_conf_softmax__2 (Activation (None, 17292, 2)      0           mbox_conf__2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 17292, 4)      0           mbox_loc__2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "predictions_tot_1 (Concatenate)  (None, 17292, 20)     0           mbox_conf__1[0][0]               \n",
      "                                                                   mbox_loc__1[0][0]                \n",
      "                                                                   mbox_priorbox__1[0][0]           \n",
      "                                                                   input_3[0][0]                    \n",
      "                                                                   input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "predictions_tot_2 (Concatenate)  (None, 17292, 20)     0           mbox_conf__2[0][0]               \n",
      "                                                                   mbox_loc__2[0][0]                \n",
      "                                                                   mbox_priorbox__2[0][0]           \n",
      "                                                                   input_4[0][0]                    \n",
      "                                                                   input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "predictions_1 (Concatenate)      (None, 17292, 18)     0           mbox_conf_softmax__1[0][0]       \n",
      "                                                                   mbox_loc__1[0][0]                \n",
      "                                                                   mbox_priorbox__1[0][0]           \n",
      "                                                                   lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "predictions_2 (Concatenate)      (None, 17292, 18)     0           mbox_conf_softmax__2[0][0]       \n",
      "                                                                   mbox_loc__2[0][0]                \n",
      "                                                                   mbox_priorbox__2[0][0]           \n",
      "                                                                   lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "predictions__1_mbox_proj (Lambda (None, 17292, 4)      0           predictions_tot_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "predictions__2_mbox_proj (Lambda (None, 17292, 4)      0           predictions_tot_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "predictions_1_proj (Concatenate) (None, 17292, 36)     0           predictions_1[0][0]              \n",
      "                                                                   mbox_conf_softmax__1[0][0]       \n",
      "                                                                   predictions__1_mbox_proj[0][0]   \n",
      "                                                                   mbox_priorbox__1[0][0]           \n",
      "                                                                   lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "predictions_2_proj (Concatenate) (None, 17292, 36)     0           predictions_2[0][0]              \n",
      "                                                                   mbox_conf_softmax__2[0][0]       \n",
      "                                                                   predictions__2_mbox_proj[0][0]   \n",
      "                                                                   mbox_priorbox__2[0][0]           \n",
      "                                                                   lambda_2[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 47,491,816\n",
      "Trainable params: 47,491,816\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred:  Tensor(\"predictions_1/concat:0\", shape=(?, 17292, 18), dtype=float32)\n",
      "y_true:  Tensor(\"predictions_1_target:0\", shape=(?, ?, ?), dtype=float32)\n",
      "y_pred:  Tensor(\"predictions_2/concat:0\", shape=(?, 17292, 18), dtype=float32)\n",
      "y_true:  Tensor(\"predictions_2_target:0\", shape=(?, ?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def gt_rem(pred, gt):\n",
    "    val = tf.subtract(tf.shape(pred)[1], tf.shape(gt)[1],name=\"gt_rem_subtract\")\n",
    "    gt = tf.slice(gt, [0, 0, 0], [1, tf.shape(pred)[1], 18],name=\"rem_slice\")\n",
    "    return gt\n",
    "\n",
    "def gt_add(pred, gt):\n",
    "    #add to gt\n",
    "    val = tf.subtract(tf.shape(pred)[1], tf.shape(gt)[1],name=\"gt_add_subtract\")\n",
    "    ext = tf.slice(gt, [0, 0, 0], [1, val, 18], name=\"add_slice\")\n",
    "    gt = K.concatenate([ext,gt], axis=1)\n",
    "    return gt\n",
    "\n",
    "def equalalready(gt, pred): return pred\n",
    "\n",
    "def make_equal(pred, gt):\n",
    "    equal_tensor = tf.cond(tf.shape(pred)[1] < tf.shape(gt)[1], lambda: gt_rem(pred, gt), lambda: gt_add(pred, gt), name=\"make_equal_cond\")\n",
    "    return equal_tensor\n",
    "\n",
    "\n",
    "\n",
    "# ssd_loss3 = SSDLoss_proj(neg_pos_ratio=3, alpha=1.0)\n",
    "# ssd_loss4 = SSDLoss_proj(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "def Accuracy(y_true, y_pred):\n",
    "    '''Calculates the mean accuracy rate across all predictions for\n",
    "    multiclass classification problems.\n",
    "    '''\n",
    "    print(\"y_pred: \",y_pred)\n",
    "    print(\"y_true: \",y_true)\n",
    "    y_true = y_true[:,:,:18]\n",
    "    y_pred = y_pred[:,:,:18]\n",
    "\n",
    "    return K.mean(K.equal(K.argmax(y_true[:,:,:-4], axis=-1),\n",
    "                  K.argmax(y_pred[:,:,:-4], axis=-1)))\n",
    "\n",
    "def Accuracy_Proj(y_pred, y_true):\n",
    "    #add to gt\n",
    "    y_true_1 = y_true[:,:,:18]\n",
    "    y_pred_1 = y_pred[:,:,:18]\n",
    "    y_true_2 = y_true[:,:,18:]\n",
    "    y_pred_2 = y_pred[:,:,18:]\n",
    "    acc = tf.constant(0)\n",
    "    y_pred, y_true = matcher(y_true_1,y_pred_1,y_true_2,y_pred_2,1)\n",
    "\n",
    "    return K.mean(K.equal(K.argmax(y_true[:,:,:-4], axis=-1),\n",
    "                  K.argmax(y_pred[:,:,:-4], axis=-1)))\n",
    "\n",
    "\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "ssd_loss1 = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "ssd_loss2 = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "ssd_loss3 = SSDLoss_proj(neg_pos_ratio=3, alpha=1.0)\n",
    "ssd_loss4 = SSDLoss_proj(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "losses = {\n",
    "    \"predictions_1\": ssd_loss1.compute_loss,\n",
    "    \"predictions_2\": ssd_loss2.compute_loss,\n",
    "    \"predictions_1_proj\": ssd_loss3.compute_loss,\n",
    "    \"predictions_2_proj\": ssd_loss4.compute_loss\n",
    "\n",
    "}\n",
    "lossWeights = {\"predictions_1\": 1.0,\"predictions_2\": 1.0,\"predictions_1_proj\": 1.0,\"predictions_2_proj\": 1.0}\n",
    "# MetricstDict = {\"predictions_1\": Accuracy,\"predictions_2\": Accuracy, \"predictions_1_proj\": Accuracy_Proj,\"predictions_2_proj\": Accuracy_Proj}\n",
    "# lossWeights = {\"predictions_1\": 1.0,\"predictions_2\": 1.0}\n",
    "MetricstDict = {\"predictions_1\": Accuracy,\"predictions_2\": Accuracy}\n",
    "\n",
    "model.compile(optimizer=adam, loss=losses, loss_weights=lossWeights, metrics=MetricstDict) \n",
    "# model.compile(optimizer=adam, loss=losses, loss_weights=lossWeights) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 300, 600, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 300, 600, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "identity_layer__1 (Lambda)       (None, 300, 600, 3)   0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "identity_layer__2 (Lambda)       (None, 300, 600, 3)   0           input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "input_mean_normalization__1 (Lam (None, 300, 600, 3)   0           identity_layer__1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "input_mean_normalization__2 (Lam (None, 300, 600, 3)   0           identity_layer__2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "input_channel_swap__1 (Lambda)   (None, 300, 600, 3)   0           input_mean_normalization__1[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "input_channel_swap__2 (Lambda)   (None, 300, 600, 3)   0           input_mean_normalization__2[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "conv1_1__1 (Conv2D)              (None, 300, 600, 64)  1792        input_channel_swap__1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1_1__2 (Conv2D)              (None, 300, 600, 64)  1792        input_channel_swap__2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1_2__1 (Conv2D)              (None, 300, 600, 64)  36928       conv1_1__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv1_2__2 (Conv2D)              (None, 300, 600, 64)  36928       conv1_1__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool1__1 (MaxPooling2D)          (None, 150, 300, 64)  0           conv1_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool1__2 (MaxPooling2D)          (None, 150, 300, 64)  0           conv1_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_1__1 (Conv2D)              (None, 150, 300, 128) 73856       pool1__1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2_1__2 (Conv2D)              (None, 150, 300, 128) 73856       pool1__2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2_2__1 (Conv2D)              (None, 150, 300, 128) 147584      conv2_1__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_2__2 (Conv2D)              (None, 150, 300, 128) 147584      conv2_1__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool2__1 (MaxPooling2D)          (None, 75, 150, 128)  0           conv2_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool2__2 (MaxPooling2D)          (None, 75, 150, 128)  0           conv2_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_1__1 (Conv2D)              (None, 75, 150, 256)  295168      pool2__1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv3_1__2 (Conv2D)              (None, 75, 150, 256)  295168      pool2__2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv3_2__1 (Conv2D)              (None, 75, 150, 256)  590080      conv3_1__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_2__2 (Conv2D)              (None, 75, 150, 256)  590080      conv3_1__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_3__1 (Conv2D)              (None, 75, 150, 256)  590080      conv3_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_3__2 (Conv2D)              (None, 75, 150, 256)  590080      conv3_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool3__1 (MaxPooling2D)          (None, 38, 75, 256)   0           conv3_3__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool3__2 (MaxPooling2D)          (None, 38, 75, 256)   0           conv3_3__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_1__1 (Conv2D)              (None, 38, 75, 512)   1180160     pool3__1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv4_1__2 (Conv2D)              (None, 38, 75, 512)   1180160     pool3__2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv4_2__1 (Conv2D)              (None, 38, 75, 512)   2359808     conv4_1__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_2__2 (Conv2D)              (None, 38, 75, 512)   2359808     conv4_1__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3__1 (Conv2D)              (None, 38, 75, 512)   2359808     conv4_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3__2 (Conv2D)              (None, 38, 75, 512)   2359808     conv4_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool4__1 (MaxPooling2D)          (None, 19, 38, 512)   0           conv4_3__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool4__2 (MaxPooling2D)          (None, 19, 38, 512)   0           conv4_3__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_1__1 (Conv2D)              (None, 19, 38, 512)   2359808     pool4__1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv5_1__2 (Conv2D)              (None, 19, 38, 512)   2359808     pool4__2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv5_2__1 (Conv2D)              (None, 19, 38, 512)   2359808     conv5_1__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_2__2 (Conv2D)              (None, 19, 38, 512)   2359808     conv5_1__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_3__1 (Conv2D)              (None, 19, 38, 512)   2359808     conv5_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_3__2 (Conv2D)              (None, 19, 38, 512)   2359808     conv5_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool5__1 (MaxPooling2D)          (None, 19, 38, 512)   0           conv5_3__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "pool5__2 (MaxPooling2D)          (None, 19, 38, 512)   0           conv5_3__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "fc6__1 (Conv2D)                  (None, 19, 38, 1024)  4719616     pool5__1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "fc6__2 (Conv2D)                  (None, 19, 38, 1024)  4719616     pool5__2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "fc7__1 (Conv2D)                  (None, 19, 38, 1024)  1049600     fc6__1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "fc7__2 (Conv2D)                  (None, 19, 38, 1024)  1049600     fc6__2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv6_1__1 (Conv2D)              (None, 19, 38, 256)   262400      fc7__1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv6_1__2 (Conv2D)              (None, 19, 38, 256)   262400      fc7__2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv6adding__1 (ZeroPadding2D)   (None, 21, 40, 256)   0           conv6_1__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv6adding__2 (ZeroPadding2D)   (None, 21, 40, 256)   0           conv6_1__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2__1 (Conv2D)              (None, 10, 19, 512)   1180160     conv6adding__1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2__2 (Conv2D)              (None, 10, 19, 512)   1180160     conv6adding__2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv7_1__1 (Conv2D)              (None, 10, 19, 128)   65664       conv6_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv7_1__2 (Conv2D)              (None, 10, 19, 128)   65664       conv6_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv7adding__1 (ZeroPadding2D)   (None, 12, 21, 128)   0           conv7_1__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv7adding__2 (ZeroPadding2D)   (None, 12, 21, 128)   0           conv7_1__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2__1 (Conv2D)              (None, 5, 10, 256)    295168      conv7adding__1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2__2 (Conv2D)              (None, 5, 10, 256)    295168      conv7adding__2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv8_1__1 (Conv2D)              (None, 5, 10, 128)    32896       conv7_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv8_1__2 (Conv2D)              (None, 5, 10, 128)    32896       conv7_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2__1 (Conv2D)              (None, 3, 8, 256)     295168      conv8_1__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2__2 (Conv2D)              (None, 3, 8, 256)     295168      conv8_1__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv9_1__1 (Conv2D)              (None, 3, 8, 128)     32896       conv8_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv9_1__2 (Conv2D)              (None, 3, 8, 128)     32896       conv8_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm__1 (L2Normalization (None, 38, 75, 512)   512         conv4_3__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2__1 (Conv2D)              (None, 1, 6, 256)     295168      conv9_1__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm__2 (L2Normalization (None, 38, 75, 512)   512         conv4_3__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2__2 (Conv2D)              (None, 1, 6, 256)     295168      conv9_1__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_loc__1 (Conv2D (None, 38, 75, 16)    73744       conv4_3_norm__1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_loc__1 (Conv2D)         (None, 19, 38, 24)    221208      fc7__1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_loc__1 (Conv2D)     (None, 10, 19, 24)    110616      conv6_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_loc__1 (Conv2D)     (None, 5, 10, 24)     55320       conv7_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_loc__1 (Conv2D)     (None, 3, 8, 16)      36880       conv8_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_loc__1 (Conv2D)     (None, 1, 6, 16)      36880       conv9_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_loc__2 (Conv2D (None, 38, 75, 16)    73744       conv4_3_norm__2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_loc__2 (Conv2D)         (None, 19, 38, 24)    221208      fc7__2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_loc__2 (Conv2D)     (None, 10, 19, 24)    110616      conv6_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_loc__2 (Conv2D)     (None, 5, 10, 24)     55320       conv7_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_loc__2 (Conv2D)     (None, 3, 8, 16)      36880       conv8_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_loc__2 (Conv2D)     (None, 1, 6, 16)      36880       conv9_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_conf__1 (Conv2 (None, 38, 75, 8)     36872       conv4_3_norm__1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_conf__1 (Conv2D)        (None, 19, 38, 12)    110604      fc7__1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_conf__1 (Conv2D)    (None, 10, 19, 12)    55308       conv6_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_conf__1 (Conv2D)    (None, 5, 10, 12)     27660       conv7_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_conf__1 (Conv2D)    (None, 3, 8, 8)       18440       conv8_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_conf__1 (Conv2D)    (None, 1, 6, 8)       18440       conv9_2__1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_priorbox__1 (A (None, 38, 75, 4, 8)  0           conv4_3_norm_mbox_loc__1[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_priorbox__1 (AnchorBoxe (None, 19, 38, 6, 8)  0           fc7_mbox_loc__1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_priorbox__1 (Anchor (None, 10, 19, 6, 8)  0           conv6_2_mbox_loc__1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_priorbox__1 (Anchor (None, 5, 10, 6, 8)   0           conv7_2_mbox_loc__1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_priorbox__1 (Anchor (None, 3, 8, 4, 8)    0           conv8_2_mbox_loc__1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_priorbox__1 (Anchor (None, 1, 6, 4, 8)    0           conv9_2_mbox_loc__1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_conf__2 (Conv2 (None, 38, 75, 8)     36872       conv4_3_norm__2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_conf__2 (Conv2D)        (None, 19, 38, 12)    110604      fc7__2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_conf__2 (Conv2D)    (None, 10, 19, 12)    55308       conv6_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_conf__2 (Conv2D)    (None, 5, 10, 12)     27660       conv7_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_conf__2 (Conv2D)    (None, 3, 8, 8)       18440       conv8_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_conf__2 (Conv2D)    (None, 1, 6, 8)       18440       conv9_2__2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_priorbox__2 (A (None, 38, 75, 4, 8)  0           conv4_3_norm_mbox_loc__2[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_priorbox__2 (AnchorBoxe (None, 19, 38, 6, 8)  0           fc7_mbox_loc__2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_priorbox__2 (Anchor (None, 10, 19, 6, 8)  0           conv6_2_mbox_loc__2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_priorbox__2 (Anchor (None, 5, 10, 6, 8)   0           conv7_2_mbox_loc__2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_priorbox__2 (Anchor (None, 3, 8, 4, 8)    0           conv8_2_mbox_loc__2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_priorbox__2 (Anchor (None, 1, 6, 4, 8)    0           conv9_2_mbox_loc__2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_conf_reshape__ (None, 11400, 2)      0           conv4_3_norm_mbox_conf__1[0][0]  \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_conf_reshape__1 (Reshap (None, 4332, 2)       0           fc7_mbox_conf__1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_conf_reshape__1 (Re (None, 1140, 2)       0           conv6_2_mbox_conf__1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_conf_reshape__1 (Re (None, 300, 2)        0           conv7_2_mbox_conf__1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_conf_reshape__1 (Re (None, 96, 2)         0           conv8_2_mbox_conf__1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_conf_reshape__1 (Re (None, 24, 2)         0           conv9_2_mbox_conf__1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_loc_reshape__1 (None, 11400, 4)      0           conv4_3_norm_mbox_loc__1[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_loc_reshape__1 (Reshape (None, 4332, 4)       0           fc7_mbox_loc__1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_loc_reshape__1 (Res (None, 1140, 4)       0           conv6_2_mbox_loc__1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_loc_reshape__1 (Res (None, 300, 4)        0           conv7_2_mbox_loc__1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_loc_reshape__1 (Res (None, 96, 4)         0           conv8_2_mbox_loc__1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_loc_reshape__1 (Res (None, 24, 4)         0           conv9_2_mbox_loc__1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_priorbox_resha (None, 11400, 8)      0           conv4_3_norm_mbox_priorbox__1[0][\n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_priorbox_reshape__1 (Re (None, 4332, 8)       0           fc7_mbox_priorbox__1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_priorbox_reshape__1 (None, 1140, 8)       0           conv6_2_mbox_priorbox__1[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_priorbox_reshape__1 (None, 300, 8)        0           conv7_2_mbox_priorbox__1[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_priorbox_reshape__1 (None, 96, 8)         0           conv8_2_mbox_priorbox__1[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_priorbox_reshape__1 (None, 24, 8)         0           conv9_2_mbox_priorbox__1[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_conf_reshape__ (None, 11400, 2)      0           conv4_3_norm_mbox_conf__2[0][0]  \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_conf_reshape__2 (Reshap (None, 4332, 2)       0           fc7_mbox_conf__2[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_conf_reshape__2 (Re (None, 1140, 2)       0           conv6_2_mbox_conf__2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_conf_reshape__2 (Re (None, 300, 2)        0           conv7_2_mbox_conf__2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_conf_reshape__2 (Re (None, 96, 2)         0           conv8_2_mbox_conf__2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_conf_reshape__2 (Re (None, 24, 2)         0           conv9_2_mbox_conf__2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_loc_reshape__2 (None, 11400, 4)      0           conv4_3_norm_mbox_loc__2[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_loc_reshape__2 (Reshape (None, 4332, 4)       0           fc7_mbox_loc__2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_loc_reshape__2 (Res (None, 1140, 4)       0           conv6_2_mbox_loc__2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_loc_reshape__2 (Res (None, 300, 4)        0           conv7_2_mbox_loc__2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_loc_reshape__2 (Res (None, 96, 4)         0           conv8_2_mbox_loc__2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_loc_reshape__2 (Res (None, 24, 4)         0           conv9_2_mbox_loc__2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_priorbox_resha (None, 11400, 8)      0           conv4_3_norm_mbox_priorbox__2[0][\n",
      "____________________________________________________________________________________________________\n",
      "fc7_mbox_priorbox_reshape__2 (Re (None, 4332, 8)       0           fc7_mbox_priorbox__2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2_mbox_priorbox_reshape__2 (None, 1140, 8)       0           conv6_2_mbox_priorbox__2[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2_mbox_priorbox_reshape__2 (None, 300, 8)        0           conv7_2_mbox_priorbox__2[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2_mbox_priorbox_reshape__2 (None, 96, 8)         0           conv8_2_mbox_priorbox__2[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2_mbox_priorbox_reshape__2 (None, 24, 8)         0           conv9_2_mbox_priorbox__2[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "mbox_conf__1 (Concatenate)       (None, 17292, 2)      0           conv4_3_norm_mbox_conf_reshape__1\n",
      "                                                                   fc7_mbox_conf_reshape__1[0][0]   \n",
      "                                                                   conv6_2_mbox_conf_reshape__1[0][0\n",
      "                                                                   conv7_2_mbox_conf_reshape__1[0][0\n",
      "                                                                   conv8_2_mbox_conf_reshape__1[0][0\n",
      "                                                                   conv9_2_mbox_conf_reshape__1[0][0\n",
      "____________________________________________________________________________________________________\n",
      "mbox_loc__1 (Concatenate)        (None, 17292, 4)      0           conv4_3_norm_mbox_loc_reshape__1[\n",
      "                                                                   fc7_mbox_loc_reshape__1[0][0]    \n",
      "                                                                   conv6_2_mbox_loc_reshape__1[0][0]\n",
      "                                                                   conv7_2_mbox_loc_reshape__1[0][0]\n",
      "                                                                   conv8_2_mbox_loc_reshape__1[0][0]\n",
      "                                                                   conv9_2_mbox_loc_reshape__1[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "mbox_priorbox__1 (Concatenate)   (None, 17292, 8)      0           conv4_3_norm_mbox_priorbox_reshap\n",
      "                                                                   fc7_mbox_priorbox_reshape__1[0][0\n",
      "                                                                   conv6_2_mbox_priorbox_reshape__1[\n",
      "                                                                   conv7_2_mbox_priorbox_reshape__1[\n",
      "                                                                   conv8_2_mbox_priorbox_reshape__1[\n",
      "                                                                   conv9_2_mbox_priorbox_reshape__1[\n",
      "____________________________________________________________________________________________________\n",
      "mbox_conf__2 (Concatenate)       (None, 17292, 2)      0           conv4_3_norm_mbox_conf_reshape__2\n",
      "                                                                   fc7_mbox_conf_reshape__2[0][0]   \n",
      "                                                                   conv6_2_mbox_conf_reshape__2[0][0\n",
      "                                                                   conv7_2_mbox_conf_reshape__2[0][0\n",
      "                                                                   conv8_2_mbox_conf_reshape__2[0][0\n",
      "                                                                   conv9_2_mbox_conf_reshape__2[0][0\n",
      "____________________________________________________________________________________________________\n",
      "mbox_loc__2 (Concatenate)        (None, 17292, 4)      0           conv4_3_norm_mbox_loc_reshape__2[\n",
      "                                                                   fc7_mbox_loc_reshape__2[0][0]    \n",
      "                                                                   conv6_2_mbox_loc_reshape__2[0][0]\n",
      "                                                                   conv7_2_mbox_loc_reshape__2[0][0]\n",
      "                                                                   conv8_2_mbox_loc_reshape__2[0][0]\n",
      "                                                                   conv9_2_mbox_loc_reshape__2[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "mbox_priorbox__2 (Concatenate)   (None, 17292, 8)      0           conv4_3_norm_mbox_priorbox_reshap\n",
      "                                                                   fc7_mbox_priorbox_reshape__2[0][0\n",
      "                                                                   conv6_2_mbox_priorbox_reshape__2[\n",
      "                                                                   conv7_2_mbox_priorbox_reshape__2[\n",
      "                                                                   conv8_2_mbox_priorbox_reshape__2[\n",
      "                                                                   conv9_2_mbox_priorbox_reshape__2[\n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 17292, 3)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 17292, 3)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "mbox_conf_softmax__1 (Activation (None, 17292, 2)      0           mbox_conf__1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 17292, 4)      0           mbox_loc__1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "mbox_conf_softmax__2 (Activation (None, 17292, 2)      0           mbox_conf__2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 17292, 4)      0           mbox_loc__2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "predictions_tot_1 (Concatenate)  (None, 17292, 20)     0           mbox_conf__1[0][0]               \n",
      "                                                                   mbox_loc__1[0][0]                \n",
      "                                                                   mbox_priorbox__1[0][0]           \n",
      "                                                                   input_3[0][0]                    \n",
      "                                                                   input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "predictions_tot_2 (Concatenate)  (None, 17292, 20)     0           mbox_conf__2[0][0]               \n",
      "                                                                   mbox_loc__2[0][0]                \n",
      "                                                                   mbox_priorbox__2[0][0]           \n",
      "                                                                   input_4[0][0]                    \n",
      "                                                                   input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "predictions_1 (Concatenate)      (None, 17292, 18)     0           mbox_conf_softmax__1[0][0]       \n",
      "                                                                   mbox_loc__1[0][0]                \n",
      "                                                                   mbox_priorbox__1[0][0]           \n",
      "                                                                   lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "predictions_2 (Concatenate)      (None, 17292, 18)     0           mbox_conf_softmax__2[0][0]       \n",
      "                                                                   mbox_loc__2[0][0]                \n",
      "                                                                   mbox_priorbox__2[0][0]           \n",
      "                                                                   lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "predictions__1_mbox_proj (Lambda (None, 17292, 4)      0           predictions_tot_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "predictions__2_mbox_proj (Lambda (None, 17292, 4)      0           predictions_tot_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "predictions_1_proj (Concatenate) (None, 17292, 36)     0           predictions_1[0][0]              \n",
      "                                                                   mbox_conf_softmax__1[0][0]       \n",
      "                                                                   predictions__1_mbox_proj[0][0]   \n",
      "                                                                   mbox_priorbox__1[0][0]           \n",
      "                                                                   lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "predictions_2_proj (Concatenate) (None, 17292, 36)     0           predictions_2[0][0]              \n",
      "                                                                   mbox_conf_softmax__2[0][0]       \n",
      "                                                                   predictions__2_mbox_proj[0][0]   \n",
      "                                                                   mbox_priorbox__2[0][0]           \n",
      "                                                                   lambda_2[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 47,491,816\n",
      "Trainable params: 47,491,816\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load a previously created model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image set 'train_few.txt': 100%|██████████| 4/4 [00:00<00:00, 23.12it/s]\n",
      "Processing image set 'val_few.txt': 100%|██████████| 2/2 [00:00<00:00, 26.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path='dataset_pascal_voc_07+12_trainval.h5')\n",
    "# val_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path='dataset_pascal_voc_07_test.h5')\n",
    "train_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "val_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "\n",
    "val_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "val_dataset_1 = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "\n",
    "\n",
    "VOC_2007_images_dir      = '../datasets/Images/'\n",
    "\n",
    "# The directories that contain the annotations.\n",
    "VOC_2007_annotations_dir      = '../datasets/VOC/Pasadena/Annotations_Multi/'\n",
    "\n",
    "VOC_2007_trainval_image_set_filename = '../datasets/VOC/Pasadena/ImageSets/Main/reid_neu/train_few.txt'\n",
    "VOC_2007_val_image_set_filename      = '../datasets/VOC/Pasadena/ImageSets/Main/reid_neu/val_few.txt'\n",
    "VOC_2007_test_image_set_filename     = '../datasets/VOC/Pasadena/ImageSets/Main/reid_neu/test_few.txt'\n",
    "\n",
    "# VOC_2007_trainval_image_set_filename = '../datasets/VOC/Pasadena/ImageSets/Main/reid_neu/train.txt'\n",
    "# VOC_2007_val_image_set_filename      = '../datasets/VOC/Pasadena/ImageSets/Main/reid_neu/val.txt'\n",
    "# VOC_2007_test_image_set_filename     = '../datasets/VOC/Pasadena/ImageSets/Main/reid_neu/test.txt'\n",
    "\n",
    "\n",
    "# The pat[Accuracy]hs to the image sets.\n",
    "# VOC_2007_trainval_image_set_filename = '../datasets/VOC/Pasadena/ImageSets/Main/siamese/trainval_sia.txt'\n",
    "# VOC_2007_val_image_set_filename      = '../datasets/VOC/Pasadena/ImageSets/Main/siamese/val_sia.txt'\n",
    "# VOC_2007_test_image_set_filename     = '../datasets/VOC/Pasadena/ImageSets/Main/siamese/test_sia.txt'\n",
    "\n",
    "# VOC_2007_trainval_image_set_filename = '../datasets/VOC/Pasadena/ImageSets/Main/siamese/trainval_sia_same.txt'\n",
    "# VOC_2007_val_image_set_filename      = '../datasets/VOC/Pasadena/ImageSets/Main/siamese/val_sia_same.txt'\n",
    "# VOC_2007_test_image_set_filename     = '../datasets/VOC/Pasadena/ImageSets/Main/siamese/test_sia_same.txt'\n",
    "\n",
    "# VOC_2007_trainval_image_set_filename = '../datasets/VOC/Pasadena/ImageSets/Main/siamese/trainval_sia_sub.txt'\n",
    "# VOC_2007_val_image_set_filename      = '../datasets/VOC/Pasadena/ImageSets/Main/siamese/val_sia_sub.txt'\n",
    "# VOC_2007_test_image_set_filename     = '../datasets/VOC/Pasadena/ImageSets/Main/siamese/test_sia_sub.txt'\n",
    "\n",
    "# VOC_2007_trainval_image_set_filename = '../datasets/VOC/Pasadena/ImageSets/Main/siamese/trainval_one.txt'\n",
    "# VOC_2007_val_image_set_filename      = '../datasets/VOC/Pasadena/ImageSets/Main/siamese/val_one.txt'\n",
    "# VOC_2007_test_image_set_filename     = '../datasets/VOC/Pasadena/ImageSets/Main/siamese/test_one.txt'\n",
    "\n",
    "# The XML parser needs to now what object class names to look for and in which order to map them to integers.\n",
    "classes = ['background',\n",
    "           'tree']\n",
    "\n",
    "train_dataset.parse_xml(images_dirs=[VOC_2007_images_dir],\n",
    "                        image_set_filenames=[VOC_2007_trainval_image_set_filename],\n",
    "                        annotations_dirs=[VOC_2007_annotations_dir],\n",
    "                        classes=classes,\n",
    "                        include_classes='all',\n",
    "                        exclude_truncated=False,\n",
    "                        exclude_difficult=False,\n",
    "                        ret=False)\n",
    "\n",
    "\n",
    "val_dataset.parse_xml(images_dirs=[VOC_2007_images_dir],\n",
    "                      image_set_filenames=[VOC_2007_val_image_set_filename],\n",
    "                      annotations_dirs=[VOC_2007_annotations_dir],\n",
    "                      classes=classes,\n",
    "                      include_classes='all',\n",
    "                      exclude_truncated=False,\n",
    "                      exclude_difficult=True,\n",
    "                      ret=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training dataset:\t     4\n",
      "Number of images in the validation dataset:\t     2\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "ssd_data_augmentation = SSDDataAugmentation_modified(img_height=img_height,\n",
    "                                            img_width=img_width,\n",
    "                                            background=mean_color)\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels_Modified()  \n",
    "resize = Resize_Modified(height=img_height, width=img_width)\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "predictor_sizes = [model.get_layer('conv4_3_norm_mbox_conf__1').output_shape[1:3],\n",
    "                   model.get_layer('fc7_mbox_conf__1').output_shape[1:3],\n",
    "                   model.get_layer('conv6_2_mbox_conf__1').output_shape[1:3],\n",
    "                   model.get_layer('conv7_2_mbox_conf__1').output_shape[1:3],\n",
    "                   model.get_layer('conv8_2_mbox_conf__1').output_shape[1:3],\n",
    "                   model.get_layer('conv9_2_mbox_conf__1').output_shape[1:3]]\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.5,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         transformations=[ssd_data_augmentation],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convert_to_3_channels,\n",
    "                                                      resize],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "\n",
    "val_dataset_size   = val_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set the remaining training parameters\n",
    "\n",
    "We've already chosen an optimizer and set the batch size above, now let's set the remaining training parameters. I'll set one epoch to consist of 1,000 training steps. The next code cell defines a learning rate schedule that replicates the learning rate schedule of the original Caffe implementation for the training of the SSD300 Pascal VOC \"07+12\" model. That model was trained for 120,000 steps with a learning rate of 0.001 for the first 80,000 steps, 0.0001 for the next 20,000 steps, and 0.00001 for the last 20,000 steps. If you're training on a different dataset, define the learning rate schedule however you see fit.\n",
    "\n",
    "I'll set only a few essential Keras callbacks below, feel free to add more callbacks if you want TensorBoard summaries or whatever. We obviously need the learning rate scheduler and we want to save the best models during the training. It also makes sense to continuously stream our training history to a CSV log file after every epoch, because if we didn't do that, in case the training terminates with an exception at some point or if the kernel of this Jupyter notebook dies for some reason or anything like that happens, we would lose the entire history for the trained epochs. Finally, we'll also add a callback that makes sure that the training terminates if the loss becomes `NaN`. Depending on the optimizer you use, it can happen that the loss becomes `NaN` during the first iterations of the training. In later iterations it's less of a risk. For example, I've never seen a `NaN` loss when I trained SSD using an Adam optimizer, but I've seen a `NaN` loss a couple of times during the very first couple of hundred training steps of training a new model when I used an SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a learning rate schedule.\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    if epoch < 80:\n",
    "        return 0.001\n",
    "    elif epoch < 100:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neg_pos_ratio = 3\n",
    "n_neg_min = 0\n",
    "alpha = 1\n",
    "\n",
    "def smooth_L1_loss(y_true, y_pred):\n",
    "    absolute_loss = tf.abs(y_true - y_pred)\n",
    "    square_loss = 0.5 * (y_true - y_pred)**2\n",
    "    l1_loss = tf.where(tf.less(absolute_loss, 1.0), square_loss, absolute_loss - 0.5)\n",
    "    return tf.reduce_sum(l1_loss, axis=-1)\n",
    "\n",
    "def log_loss(y_true, y_pred):\n",
    "\n",
    "    y_pred = tf.maximum(y_pred, 1e-15)\n",
    "    # Compute the log loss\n",
    "    log_loss = -tf.reduce_sum(y_true * tf.log(y_pred), axis=-1)\n",
    "    return log_loss\n",
    "\n",
    "\n",
    "def compute_loss(y_true, y_pred):\n",
    "    def gt_rem(pred, gt):\n",
    "        val = tf.subtract(tf.shape(pred)[1], tf.shape(gt)[1],name=\"gt_rem_subtract\")\n",
    "        gt = tf.slice(gt, [0, 0, 0], [1, tf.shape(pred)[1], 18],name=\"rem_slice\")\n",
    "        return gt\n",
    "\n",
    "    def gt_add(pred, gt):\n",
    "        #add to gt\n",
    "        val = tf.subtract(tf.shape(pred)[1], tf.shape(gt)[1],name=\"gt_add_subtract\")\n",
    "        ext = tf.slice(gt, [0, 0, 0], [1, val, 18], name=\"add_slice\")\n",
    "        gt = K.concatenate([ext,gt], axis=1)\n",
    "        return gt\n",
    "\n",
    "    def equalalready(gt, pred): return pred\n",
    "\n",
    "    def make_equal(pred, gt):\n",
    "        equal_tensor = tf.cond(tf.shape(pred)[1] < tf.shape(gt)[1], lambda: gt_rem(pred, gt), lambda: gt_add(pred, gt), name=\"make_equal_cond\")\n",
    "        return equal_tensor\n",
    "\n",
    "\n",
    "    def matcher(y_true_1,y_true_2,y_pred_1,y_pred_2, bsz):\n",
    "        pred = 0\n",
    "        gt = 0\n",
    "        print(\"@@@: \",y_true_2[:,20:50,:])\n",
    "\n",
    "        for i in range(bsz):\n",
    "            \n",
    "            filterer = tf.where(tf.not_equal(y_true_1[i,:,-4],99))\n",
    "            filterer_2 = tf.where(tf.not_equal(y_true_2[i,:,-4],99))\n",
    "            print(\"$$$: \",y_true_2[:,20:50,:])\n",
    "\n",
    "            y_true_new = tf.gather_nd(y_true_1[i,:,:],filterer)            \n",
    "            y_true_new = tf.expand_dims(y_true_new, 0)\n",
    "            \n",
    "            y_true_2_new = tf.gather_nd(y_true_2[i,:,:],filterer_2)\n",
    "            y_true_2_new = tf.expand_dims(y_true_2_new, 0)\n",
    "\n",
    "            set1 = y_true_new[i,:,-4]\n",
    "            set2 = y_true_2_new[i,:,-4]\n",
    "#             print(K.eval(y_true_2_new[i,20:50,:]))\n",
    "            print(K.eval(set1))\n",
    "            print(K.eval(set2))\n",
    "\n",
    "#             s = tf.sets.set_intersection(set1[None,:], set2[None, :])\n",
    "            \n",
    "            iou_out = tf.py_func(iou, [y_pred_1[i,:,-16:-12],tf.convert_to_tensor(y_true_new[i,:,-16:-12])], tf.float64, name=\"iou_out\")\n",
    "            bipartite_matches = tf.py_func(match_bipartite_greedy, [iou_out], tf.int64, name=\"bipartite_matches\")\n",
    "            out = tf.gather(y_pred_2[i,:,:], [bipartite_matches], axis=0, name=\"out\")\n",
    "            \n",
    "\n",
    "\n",
    "            box_comparer = tf.reduce_all(tf.equal(tf.shape(out)[1], tf.shape(y_true_2_new)[1]), name=\"box_comparer\")\n",
    "            y_true_2_equal = tf.cond(box_comparer, lambda: equalalready(out, y_true_2_new), lambda: make_equal(out, y_true_2_new), name=\"y_true_cond\")\n",
    "\n",
    "            if i != 0:\n",
    "                pred = K.concatenate([pred,out], axis=-1)\n",
    "                gt = K.concatenate([gt,y_true_2_equal], axis=0)\n",
    "            else:\n",
    "                pred = out\n",
    "                gt = y_true_2_equal    \n",
    "        return pred, gt\n",
    "    \n",
    "    y_true_1 = y_true[:,:,:18]\n",
    "    y_pred_1 = y_pred[:,:,:18]\n",
    "    y_true_2 = y_true[:,:,18:]\n",
    "    y_pred_2 = y_pred[:,:,18:]\n",
    "    print(\"###: \",y_true_2[:,20:50,:])\n",
    "\n",
    "    y_pred, y_true = matcher(y_true_1,y_pred_1,y_true_2,y_pred_2,1)\n",
    "    y_pred1 = y_pred_1\n",
    "    t_true1 = y_true_1\n",
    "\n",
    "    batch_size = tf.shape(y_pred1)[0]\n",
    "    n_boxes = tf.shape(t_true1)[1] \n",
    "\n",
    "    classification_loss = tf.to_float(log_loss(t_true1[:,:,:-16], y_pred1[:,:,:-16])) # Output shape: (batch_size, n_boxes)\n",
    "    localization_loss = tf.to_float(smooth_L1_loss(t_true1[:,:,-16:-12], y_pred1[:,:,-16:-12])) # Output shape: (batch_size, n_boxes)\n",
    "\n",
    "    negatives = t_true1[:,:,0] # Tensor of shape (batch_size, n_boxes)\n",
    "    positives = tf.to_float(tf.reduce_max(t_true1[:,:,1:-16], axis=-1)) # Tensor of shape (batch_size, n_boxes)\n",
    "    n_positive = tf.reduce_sum(positives)\n",
    "\n",
    "    pos_class_loss = tf.reduce_sum(classification_loss * positives, axis=-1) # Tensor of shape (batch_size,)\n",
    "\n",
    "\n",
    "    neg_class_loss_all = classification_loss * negatives # Tensor of shape (batch_size, n_boxes)\n",
    "    n_neg_losses = tf.count_nonzero(neg_class_loss_all, dtype=tf.int32) # The number of non-zero loss entries in `neg_class_loss_all`\n",
    "    n_negative_keep = tf.minimum(tf.maximum(neg_pos_ratio * tf.to_int32(n_positive), n_neg_min), n_neg_losses)\n",
    "\n",
    "    def f1():\n",
    "        return tf.zeros([batch_size])\n",
    "    def f2():\n",
    "\n",
    "        neg_class_loss_all_1D = tf.reshape(neg_class_loss_all, [-1]) # Tensor of shape (batch_size * n_boxes,)\n",
    "        values, indices = tf.nn.top_k(neg_class_loss_all_1D,\n",
    "                                      k=n_negative_keep,\n",
    "                                      sorted=False) # We don't need them sorted.\n",
    "\n",
    "        negatives_keep = tf.scatter_nd(indices=tf.expand_dims(indices, axis=1),\n",
    "                                       updates=tf.ones_like(indices, dtype=tf.int32),\n",
    "                                       shape=tf.shape(neg_class_loss_all_1D)) # Tensor of shape (batch_size * n_boxes,)\n",
    "        negatives_keep = tf.to_float(tf.reshape(negatives_keep, [batch_size, n_boxes])) # Tensor of shape (batch_size, n_boxes)\n",
    "        # ...and use it to keep only those boxes and mask all other classification losses\n",
    "        neg_class_loss = tf.reduce_sum(classification_loss * negatives_keep, axis=-1) # Tensor of shape (batch_size,)\n",
    "        return neg_class_loss\n",
    "\n",
    "    neg_class_loss = tf.cond(tf.equal(n_neg_losses, tf.constant(0)), f1, f2)\n",
    "\n",
    "    class_loss = pos_class_loss + neg_class_loss # Tensor of shape (batch_size,)\n",
    "\n",
    "    loc_loss = tf.reduce_sum(localization_loss * positives, axis=-1) # Tensor of shape (batch_size,)\n",
    "\n",
    "    # 4: Compute the total loss.\n",
    "\n",
    "    total_loss = (class_loss + alpha * loc_loss) / tf.maximum(1.0, n_positive) # In case `n_positive == 0`\n",
    "    total_loss = total_loss * tf.to_float(batch_size)\n",
    "    total_loss.set_shape((None,))\n",
    "    return total_loss, y_pred, y_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prediction_history(Callback):\n",
    "    def __init__(self):\n",
    "        print(\"Predictor\")\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        predder = np.load('outputs/predder.npy')\n",
    "        bX = predder[0][0]\n",
    "        bZ = predder[0][1]\n",
    "        gX = predder[0][2]\n",
    "        gZ = predder[0][3]\n",
    "        \n",
    "        y_true = predder[1]['predictions_1_proj']\n",
    "\n",
    "        intermediate_layer_model = Model(inputs=model.input,\n",
    "                             outputs=model.get_layer(\"predictions_1\").output)\n",
    "        intermediate_layer_model_1 = Model(inputs=model.input,\n",
    "                             outputs=model.get_layer(\"predictions_1_proj\").output)\n",
    "        intermediate_layer_model_2 = Model(inputs=model.input,\n",
    "                             outputs=model.get_layer(\"predictions_2\").output)\n",
    "        intermediate_layer_model_3 = Model(inputs=model.input,\n",
    "                             outputs=model.get_layer(\"predictions_2_proj\").output)\n",
    "\n",
    "        intermediate_output = intermediate_layer_model.predict([bX,bZ,gX,gZ])\n",
    "        intermediate_output_1 = intermediate_layer_model_1.predict([bX,bZ,gX,gZ])\n",
    "        intermediate_output_2 = intermediate_layer_model_2.predict([bX,bZ,gX,gZ])\n",
    "        intermediate_output_3 = intermediate_layer_model_3.predict([bX,bZ,gX,gZ])\n",
    "        loss,y_pred1, y_true1 = compute_loss(y_true,intermediate_output_1)\n",
    "        \n",
    "        np.save('outputs/y_pred'+str(epoch)+'.npy',K.eval(y_pred1))\n",
    "        np.save('outputs/y_true'+str(epoch)+'.npy',K.eval(y_true1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor\n"
     ]
    }
   ],
   "source": [
    "# Define model callbacks.\n",
    "\n",
    "# TODO: Set the filepath under which you want to save the model.\n",
    "model_checkpoint = ModelCheckpoint(filepath='checkpoints/double_ssd300_pascal_07+12_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "#model_checkpoint.best = \n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "csv_logger = CSVLogger(filename='ssd300_pascal_07+12_training_log.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "learning_rate_scheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=1,\n",
    "                              verbose=0, mode='auto')\n",
    "\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "printer_callback = prediction_history()\n",
    "# custom_los = custom_loss()\n",
    "callbacks = [\n",
    "#             model_checkpoint,\n",
    "#             csv_logger,\n",
    "#             custom_los,\n",
    "            learning_rate_scheduler,\n",
    "            early_stopping,\n",
    "            terminate_on_nan,\n",
    "            printer_callback,\n",
    "            tbCallBack\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "###:  [[[1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 7.33333333e-02 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 7.33333333e-02 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 7.33333333e-02 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 7.33333333e-02 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 8.66666667e-02 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 8.66666667e-02 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 8.66666667e-02 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 8.66666667e-02 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.00000000e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.00000000e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.00000000e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.00000000e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.13333333e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.13333333e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.13333333e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.13333333e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.26666667e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.26666667e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.26666667e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.26666667e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.40000000e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.40000000e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.40000000e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.40000000e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.53333333e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.53333333e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.53333333e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.53333333e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.66666667e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.66666667e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]]\n",
      "\n",
      " [[1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 7.33333333e-02 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 7.33333333e-02 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 7.33333333e-02 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 7.33333333e-02 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 8.66666667e-02 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 8.66666667e-02 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 8.66666667e-02 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 8.66666667e-02 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.00000000e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.00000000e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.00000000e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.00000000e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.13333333e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.13333333e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.13333333e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.13333333e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.26666667e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.26666667e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.26666667e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.26666667e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.40000000e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.40000000e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.40000000e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.40000000e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.53333333e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.53333333e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.53333333e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.53333333e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.66666667e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.66666667e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]]]\n",
      "@@@:  [[[ 0.24135943  0.75864065  0.21986249 -0.69142646  0.30512804\n",
      "   -1.7859298   0.07333333  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.12145667  0.8785434  -0.49715987 -0.22229113  0.7184436\n",
      "   -0.24494782  0.07333333  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.38394657  0.61605346  0.46684664 -2.0236795  -2.2163956\n",
      "    0.30474195  0.07333333  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.14460593  0.855394   -0.6405002  -0.20589797 -0.9691683\n",
      "    0.0244524   0.07333333  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.23977055  0.7602294   0.23955977 -0.6850663   0.3094908\n",
      "   -1.7622527   0.08666667  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.12252723  0.87747276 -0.50843537 -0.21383587  0.7081909\n",
      "   -0.24592201  0.08666667  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.39038146  0.60961854  0.45336872 -2.024218   -2.2127204\n",
      "    0.30154556  0.08666667  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.1483662   0.8516338  -0.626871   -0.23715347 -0.95963526\n",
      "    0.02005144  0.08666667  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.24208398  0.757916    0.20995216 -0.67931205  0.32753\n",
      "   -1.7821147   0.1         0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.1205322   0.8794678  -0.5135301  -0.22116365  0.7063731\n",
      "   -0.23886944  0.1         0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.38974094  0.6102591   0.45084772 -2.0039637  -2.213877\n",
      "    0.308432    0.1         0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.14933182  0.85066813 -0.609523   -0.23733963 -0.9363348\n",
      "    0.03213829  0.1         0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.2390081   0.7609919   0.2251176  -0.6891812   0.33910006\n",
      "   -1.7833809   0.11333334  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.12320231  0.87679774 -0.505866   -0.23283112  0.73672193\n",
      "   -0.23782502  0.11333334  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.38815102  0.611849    0.44747847 -2.030287   -2.2136512\n",
      "    0.31321004  0.11333334  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.15026785  0.84973216 -0.62349886 -0.23079255 -0.9211695\n",
      "    0.02886612  0.11333334  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.23793615  0.7620638   0.2525601  -0.660164    0.3467431\n",
      "   -1.8242084   0.12666667  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.12992308  0.8700769  -0.4726543  -0.24941035  0.7532554\n",
      "   -0.27735913  0.12666667  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.38857067  0.6114294   0.503395   -2.027074   -2.22837\n",
      "    0.29970396  0.12666667  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.15108524  0.84891474 -0.6353474  -0.23778115 -0.9166774\n",
      "    0.00708414  0.12666667  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.24071276  0.75928724  0.24712768 -0.6636235   0.3299406\n",
      "   -1.8331705   0.14        0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.13094376  0.8690562  -0.47393465 -0.23000349  0.7278696\n",
      "   -0.30303046  0.14        0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.38342845  0.61657155  0.48843932 -2.0075107  -2.2105722\n",
      "    0.32963294  0.14        0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.15126978  0.84873027 -0.6381455  -0.23592795 -0.90029913\n",
      "    0.00277245  0.14        0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.24007912  0.7599209   0.25048515 -0.6715745   0.319864\n",
      "   -1.8260058   0.15333334  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.13174698  0.86825305 -0.47465587 -0.20742494  0.7181095\n",
      "   -0.313505    0.15333334  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.3795609   0.62043905  0.46513444 -1.9885504  -2.2023997\n",
      "    0.33780822  0.15333334  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.15052909  0.84947085 -0.650152   -0.21718758 -0.9248685\n",
      "   -0.00995294  0.15333334  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.24269144  0.7573086   0.2451953  -0.68206656  0.32071617\n",
      "   -1.8297647   0.16666667  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.12803791  0.8719621  -0.49985746 -0.19412825  0.7160741\n",
      "   -0.29150847  0.16666667  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]]\n",
      "\n",
      " [[ 0.20926592  0.79073405  0.14699799 -0.62976     0.10461728\n",
      "   -1.7596312   0.07333333  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.17766343  0.8223366  -0.6420938  -0.42122513  0.77014935\n",
      "    0.02485371  0.07333333  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.35932276  0.6406772   0.4161022  -1.9282558  -1.936529\n",
      "    0.34614602  0.07333333  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.14988646  0.8501136  -0.5488031  -0.36040434 -0.91520953\n",
      "    0.14662549  0.07333333  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.20743111  0.7925689   0.1566007  -0.6297383   0.10243484\n",
      "   -1.7816458   0.08666667  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.17850745  0.82149255 -0.63347405 -0.4209911   0.77174056\n",
      "    0.02425553  0.08666667  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.3630505   0.6369495   0.4102215  -1.9231892  -1.9240508\n",
      "    0.32895064  0.08666667  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.15190056  0.84809947 -0.5590235  -0.36221132 -0.92639583\n",
      "    0.1502016   0.08666667  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.20607553  0.79392445  0.14525208 -0.6285761   0.10782015\n",
      "   -1.7616616   0.1         0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.18158768  0.8184123  -0.64679474 -0.4217178   0.77605236\n",
      "    0.02169254  0.1         0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.3641718   0.63582814  0.44649684 -1.9283689  -1.9378762\n",
      "    0.34953716  0.1         0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.15424398  0.845756   -0.55107546 -0.3830091  -0.92337453\n",
      "    0.1352124   0.1         0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.21020623  0.7897937   0.15600319 -0.62972707  0.11829406\n",
      "   -1.7755027   0.11333334  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.1827198   0.8172802  -0.6571899  -0.42954373  0.782261\n",
      "    0.01532216  0.11333334  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.36076197  0.639238    0.45327076 -1.9278053  -1.944735\n",
      "    0.36492962  0.11333334  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.1556638   0.8443362  -0.5546116  -0.3711581  -0.91496015\n",
      "    0.13300654  0.11333334  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.21280907  0.7871909   0.13094865 -0.63508624  0.12925392\n",
      "   -1.804021    0.12666667  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.18538283  0.81461716 -0.64413834 -0.42197764  0.78514844\n",
      "    0.01984716  0.12666667  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.35927618  0.6407238   0.44666526 -1.9297378  -1.9529791\n",
      "    0.36363626  0.12666667  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.15675189  0.8432481  -0.56855255 -0.36239707 -0.8912699\n",
      "    0.14319536  0.12666667  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.21193996  0.78806     0.1344224  -0.65470344  0.11683392\n",
      "   -1.8078797   0.14        0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.18336901  0.81663096 -0.65602905 -0.44588575  0.7845142\n",
      "    0.01319663  0.14        0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.35696754  0.6430325   0.44012788 -1.9121132  -1.9275904\n",
      "    0.3656247   0.14        0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.15441418  0.8455858  -0.5735313  -0.35678193 -0.89253235\n",
      "    0.15086828  0.14        0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.21509244  0.7849075   0.13833804 -0.6550531   0.13325445\n",
      "   -1.8021219   0.15333334  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.18323559  0.81676435 -0.6483747  -0.44980383  0.80660886\n",
      "    0.00833318  0.15333334  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.35337606  0.6466239   0.4488118  -1.9121277  -1.9217689\n",
      "    0.34489745  0.15333334  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.15506786  0.84493214 -0.5748754  -0.3460988  -0.89689326\n",
      "    0.15389563  0.15333334  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.21742463  0.7825753   0.14078215 -0.65166473  0.10845492\n",
      "   -1.8112048   0.16666667  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.18134126  0.81865877 -0.6403721  -0.44431147  0.7973034\n",
      "    0.00828118  0.16666667  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]]]\n",
      "$$$:  [[[ 0.24135943  0.75864065  0.21986249 -0.69142646  0.30512804\n",
      "   -1.7859298   0.07333333  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.12145667  0.8785434  -0.49715987 -0.22229113  0.7184436\n",
      "   -0.24494782  0.07333333  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.38394657  0.61605346  0.46684664 -2.0236795  -2.2163956\n",
      "    0.30474195  0.07333333  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.14460593  0.855394   -0.6405002  -0.20589797 -0.9691683\n",
      "    0.0244524   0.07333333  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.23977055  0.7602294   0.23955977 -0.6850663   0.3094908\n",
      "   -1.7622527   0.08666667  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.12252723  0.87747276 -0.50843537 -0.21383587  0.7081909\n",
      "   -0.24592201  0.08666667  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.39038146  0.60961854  0.45336872 -2.024218   -2.2127204\n",
      "    0.30154556  0.08666667  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.1483662   0.8516338  -0.626871   -0.23715347 -0.95963526\n",
      "    0.02005144  0.08666667  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.24208398  0.757916    0.20995216 -0.67931205  0.32753\n",
      "   -1.7821147   0.1         0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.1205322   0.8794678  -0.5135301  -0.22116365  0.7063731\n",
      "   -0.23886944  0.1         0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.38974094  0.6102591   0.45084772 -2.0039637  -2.213877\n",
      "    0.308432    0.1         0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.14933182  0.85066813 -0.609523   -0.23733963 -0.9363348\n",
      "    0.03213829  0.1         0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.2390081   0.7609919   0.2251176  -0.6891812   0.33910006\n",
      "   -1.7833809   0.11333334  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.12320231  0.87679774 -0.505866   -0.23283112  0.73672193\n",
      "   -0.23782502  0.11333334  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.38815102  0.611849    0.44747847 -2.030287   -2.2136512\n",
      "    0.31321004  0.11333334  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.15026785  0.84973216 -0.62349886 -0.23079255 -0.9211695\n",
      "    0.02886612  0.11333334  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.23793615  0.7620638   0.2525601  -0.660164    0.3467431\n",
      "   -1.8242084   0.12666667  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.12992308  0.8700769  -0.4726543  -0.24941035  0.7532554\n",
      "   -0.27735913  0.12666667  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.38857067  0.6114294   0.503395   -2.027074   -2.22837\n",
      "    0.29970396  0.12666667  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.15108524  0.84891474 -0.6353474  -0.23778115 -0.9166774\n",
      "    0.00708414  0.12666667  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.24071276  0.75928724  0.24712768 -0.6636235   0.3299406\n",
      "   -1.8331705   0.14        0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.13094376  0.8690562  -0.47393465 -0.23000349  0.7278696\n",
      "   -0.30303046  0.14        0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.38342845  0.61657155  0.48843932 -2.0075107  -2.2105722\n",
      "    0.32963294  0.14        0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.15126978  0.84873027 -0.6381455  -0.23592795 -0.90029913\n",
      "    0.00277245  0.14        0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.24007912  0.7599209   0.25048515 -0.6715745   0.319864\n",
      "   -1.8260058   0.15333334  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.13174698  0.86825305 -0.47465587 -0.20742494  0.7181095\n",
      "   -0.313505    0.15333334  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.3795609   0.62043905  0.46513444 -1.9885504  -2.2023997\n",
      "    0.33780822  0.15333334  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.15052909  0.84947085 -0.650152   -0.21718758 -0.9248685\n",
      "   -0.00995294  0.15333334  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.24269144  0.7573086   0.2451953  -0.68206656  0.32071617\n",
      "   -1.8297647   0.16666667  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.12803791  0.8719621  -0.49985746 -0.19412825  0.7160741\n",
      "   -0.29150847  0.16666667  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]]\n",
      "\n",
      " [[ 0.20926592  0.79073405  0.14699799 -0.62976     0.10461728\n",
      "   -1.7596312   0.07333333  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.17766343  0.8223366  -0.6420938  -0.42122513  0.77014935\n",
      "    0.02485371  0.07333333  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.35932276  0.6406772   0.4161022  -1.9282558  -1.936529\n",
      "    0.34614602  0.07333333  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.14988646  0.8501136  -0.5488031  -0.36040434 -0.91520953\n",
      "    0.14662549  0.07333333  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.20743111  0.7925689   0.1566007  -0.6297383   0.10243484\n",
      "   -1.7816458   0.08666667  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.17850745  0.82149255 -0.63347405 -0.4209911   0.77174056\n",
      "    0.02425553  0.08666667  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.3630505   0.6369495   0.4102215  -1.9231892  -1.9240508\n",
      "    0.32895064  0.08666667  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.15190056  0.84809947 -0.5590235  -0.36221132 -0.92639583\n",
      "    0.1502016   0.08666667  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.20607553  0.79392445  0.14525208 -0.6285761   0.10782015\n",
      "   -1.7616616   0.1         0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.18158768  0.8184123  -0.64679474 -0.4217178   0.77605236\n",
      "    0.02169254  0.1         0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.3641718   0.63582814  0.44649684 -1.9283689  -1.9378762\n",
      "    0.34953716  0.1         0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.15424398  0.845756   -0.55107546 -0.3830091  -0.92337453\n",
      "    0.1352124   0.1         0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.21020623  0.7897937   0.15600319 -0.62972707  0.11829406\n",
      "   -1.7755027   0.11333334  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.1827198   0.8172802  -0.6571899  -0.42954373  0.782261\n",
      "    0.01532216  0.11333334  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.36076197  0.639238    0.45327076 -1.9278053  -1.944735\n",
      "    0.36492962  0.11333334  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.1556638   0.8443362  -0.5546116  -0.3711581  -0.91496015\n",
      "    0.13300654  0.11333334  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.21280907  0.7871909   0.13094865 -0.63508624  0.12925392\n",
      "   -1.804021    0.12666667  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.18538283  0.81461716 -0.64413834 -0.42197764  0.78514844\n",
      "    0.01984716  0.12666667  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.35927618  0.6407238   0.44666526 -1.9297378  -1.9529791\n",
      "    0.36363626  0.12666667  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.15675189  0.8432481  -0.56855255 -0.36239707 -0.8912699\n",
      "    0.14319536  0.12666667  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.21193996  0.78806     0.1344224  -0.65470344  0.11683392\n",
      "   -1.8078797   0.14        0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.18336901  0.81663096 -0.65602905 -0.44588575  0.7845142\n",
      "    0.01319663  0.14        0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.35696754  0.6430325   0.44012788 -1.9121132  -1.9275904\n",
      "    0.3656247   0.14        0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.15441418  0.8455858  -0.5735313  -0.35678193 -0.89253235\n",
      "    0.15086828  0.14        0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.21509244  0.7849075   0.13833804 -0.6550531   0.13325445\n",
      "   -1.8021219   0.15333334  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.18323559  0.81676435 -0.6483747  -0.44980383  0.80660886\n",
      "    0.00833318  0.15333334  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.35337606  0.6466239   0.4488118  -1.9121277  -1.9217689\n",
      "    0.34489745  0.15333334  0.01333333  0.07071068  0.07071068\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.15506786  0.84493214 -0.5748754  -0.3460988  -0.89689326\n",
      "    0.15389563  0.15333334  0.01333333  0.03535534  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.21742463  0.7825753   0.14078215 -0.65166473  0.10845492\n",
      "   -1.8112048   0.16666667  0.01333333  0.05        0.1\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]\n",
      "  [ 0.18134126  0.81865877 -0.6403721  -0.44431147  0.7973034\n",
      "    0.00828118  0.16666667  0.01333333  0.07071068  0.14142136\n",
      "    0.1         0.1         0.2         0.2         1.\n",
      "    1.          1.          1.        ]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100055. 100055. 100055. 100055. 100055. 100055. 100055. 100055.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 70s - loss: 568.7474 - predictions_1_loss: 147.0945 - predictions_2_loss: 130.8042 - predictions_1_proj_loss: 147.0945 - predictions_2_proj_loss: 130.8042 - predictions_1_Accuracy: 0.1731 - predictions_2_Accuracy: 0.1742 - val_loss: 3039.3665 - val_predictions_1_loss: 291.2819 - val_predictions_2_loss: 1221.9869 - val_predictions_1_proj_loss: 291.2819 - val_predictions_2_proj_loss: 1221.9869 - val_predictions_1_Accuracy: 0.0194 - val_predictions_2_Accuracy: 0.0033\n",
      "Epoch 2/500\n",
      "###:  [[[1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 7.33333333e-02 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 7.33333333e-02 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 7.33333333e-02 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 7.33333333e-02 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 8.66666667e-02 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 8.66666667e-02 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 8.66666667e-02 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 8.66666667e-02 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.00000000e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.00000000e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.00000000e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.00000000e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.13333333e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.13333333e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.13333333e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.13333333e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.26666667e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.26666667e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.26666667e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.26666667e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.40000000e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.40000000e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.40000000e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.40000000e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.53333333e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.53333333e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.53333333e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.53333333e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.66666667e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.66666667e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]]\n",
      "\n",
      " [[1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 7.33333333e-02 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 7.33333333e-02 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 7.33333333e-02 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 7.33333333e-02 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 8.66666667e-02 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 8.66666667e-02 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 8.66666667e-02 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 8.66666667e-02 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.00000000e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.00000000e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.00000000e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.00000000e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.13333333e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.13333333e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.13333333e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.13333333e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.26666667e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.26666667e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.26666667e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.26666667e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.40000000e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.40000000e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.40000000e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.40000000e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.53333333e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.53333333e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.53333333e-01 1.33333333e-02\n",
      "   7.07106781e-02 7.07106781e-02 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.53333333e-01 1.33333333e-02\n",
      "   3.53553391e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.66666667e-01 1.33333333e-02\n",
      "   5.00000000e-02 1.00000000e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.66666667e-01 1.33333333e-02\n",
      "   7.07106781e-02 1.41421356e-01 1.00000000e-01 1.00000000e-01\n",
      "   2.00000000e-01 2.00000000e-01 9.90000000e+01 9.90000000e+01\n",
      "   1.99000000e+02 1.99000000e+02]]]\n",
      "@@@:  [[[ 1.77775081e-02  9.82222438e-01 -1.54534757e-01 -3.30899775e-01\n",
      "   -4.83035296e-02 -1.54294872e+00  7.33333305e-02  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 3.99080701e-02  9.60091889e-01 -6.14868760e-01 -5.72747469e-01\n",
      "   -1.50855231e+00 -6.31954610e-01  7.33333305e-02  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 9.56612751e-02  9.04338717e-01  2.66534716e-01 -1.57088614e+00\n",
      "   -2.10070992e+00  2.11124539e-01  7.33333305e-02  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.20739722e-01  7.79260337e-01 -3.34144495e-02  7.95117021e-01\n",
      "   -7.22740233e-01 -8.68142724e-01  7.33333305e-02  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.83213372e-02  9.81678665e-01 -1.43269196e-01 -3.29212666e-01\n",
      "   -4.48220633e-02 -1.55283928e+00  8.66666660e-02  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 3.86100374e-02  9.61389959e-01 -5.94680667e-01 -5.78743219e-01\n",
      "   -1.55287826e+00 -5.94132125e-01  8.66666660e-02  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 9.89340842e-02  9.01065886e-01  2.75775701e-01 -1.56212163e+00\n",
      "   -2.16302013e+00  1.77692249e-01  8.66666660e-02  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.17326149e-01  7.82673836e-01 -7.58128315e-02  8.14614534e-01\n",
      "   -7.25522935e-01 -9.03843462e-01  8.66666660e-02  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.77599825e-02  9.82240021e-01 -1.79930776e-01 -3.45875174e-01\n",
      "   -5.68329655e-02 -1.48679543e+00  1.00000001e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 3.97404321e-02  9.60259616e-01 -5.52096963e-01 -5.65840304e-01\n",
      "   -1.62767184e+00 -6.13561094e-01  1.00000001e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 9.77679193e-02  9.02232051e-01  2.40275547e-01 -1.57735610e+00\n",
      "   -2.18811274e+00  1.95738301e-01  1.00000001e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.27344185e-01  7.72655785e-01 -4.79611643e-02  7.91916549e-01\n",
      "   -7.25297689e-01 -9.09973264e-01  1.00000001e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.81830265e-02  9.81817007e-01 -1.99204713e-01 -2.71107614e-01\n",
      "   -1.12735800e-01 -1.54326272e+00  1.13333337e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 4.04887348e-02  9.59511280e-01 -6.13152146e-01 -5.96778989e-01\n",
      "   -1.65840042e+00 -5.59940815e-01  1.13333337e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 9.09142867e-02  9.09085691e-01  2.11079806e-01 -1.60902619e+00\n",
      "   -2.19505596e+00  1.67958617e-01  1.13333337e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.15871543e-01  7.84128487e-01 -3.27129439e-02  7.20090389e-01\n",
      "   -7.82006264e-01 -8.90745878e-01  1.13333337e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.84412412e-02  9.81558740e-01 -1.52084276e-01 -2.54050434e-01\n",
      "   -7.48397857e-02 -1.52516401e+00  1.26666665e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 4.21048217e-02  9.57895160e-01 -6.02942169e-01 -5.45592308e-01\n",
      "   -1.66208172e+00 -6.13999724e-01  1.26666665e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 8.44232589e-02  9.15576816e-01  1.77335963e-01 -1.66553819e+00\n",
      "   -2.14661169e+00  1.47645548e-01  1.26666665e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.31650665e-01  7.68349349e-01 -6.26221597e-02  7.66780734e-01\n",
      "   -7.61083186e-01 -9.55631196e-01  1.26666665e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.73662715e-02  9.82633770e-01 -1.33196756e-01 -2.23185658e-01\n",
      "   -4.15818021e-02 -1.52196395e+00  1.40000001e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 4.09712307e-02  9.59028721e-01 -6.49072886e-01 -5.26493073e-01\n",
      "   -1.62471640e+00 -6.60888791e-01  1.40000001e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 8.91312808e-02  9.10868704e-01  1.82210535e-01 -1.70270824e+00\n",
      "   -2.11640644e+00  1.93654552e-01  1.40000001e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.23342001e-01  7.76657939e-01 -1.58982724e-02  7.52505779e-01\n",
      "   -7.98226297e-01 -9.12950754e-01  1.40000001e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.67602934e-02  9.83239710e-01 -1.48706183e-01 -2.15952486e-01\n",
      "    2.06813519e-03 -1.51590109e+00  1.53333336e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 4.28599715e-02  9.57140028e-01 -6.81288302e-01 -5.07862031e-01\n",
      "   -1.55680752e+00 -6.99294627e-01  1.53333336e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 8.40896294e-02  9.15910363e-01  2.20154807e-01 -1.65382087e+00\n",
      "   -2.08299136e+00  2.05536351e-01  1.53333336e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.17596307e-01  7.82403708e-01 -2.00031549e-02  7.70256758e-01\n",
      "   -7.73673654e-01 -9.68867779e-01  1.53333336e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.66150965e-02  9.83384967e-01 -1.21835217e-01 -2.36120448e-01\n",
      "    4.40971553e-03 -1.56558800e+00  1.66666672e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 3.83719318e-02  9.61628139e-01 -6.65790081e-01 -5.38364172e-01\n",
      "   -1.60497832e+00 -6.81910276e-01  1.66666672e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]]\n",
      "\n",
      " [[ 1.89123116e-02  9.81087685e-01 -9.87609848e-03 -3.45876776e-02\n",
      "   -1.81206986e-01 -1.24198472e+00  7.33333305e-02  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 5.78275919e-02  9.42172348e-01 -1.28227496e+00 -5.34978092e-01\n",
      "   -1.60203087e+00 -6.28040314e-01  7.33333305e-02  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.17645346e-02  9.38235521e-01 -3.91072243e-01 -1.43466687e+00\n",
      "   -1.34473944e+00  6.11802459e-01  7.33333305e-02  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.40254104e-01  7.59745896e-01 -1.83085486e-01  4.68459427e-01\n",
      "   -9.46856260e-01 -8.50521266e-01  7.33333305e-02  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.90541819e-02  9.80945766e-01 -4.57444275e-03 -3.96661796e-02\n",
      "   -1.97084174e-01 -1.22762072e+00  8.66666660e-02  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 5.81884272e-02  9.41811562e-01 -1.26513696e+00 -5.39531112e-01\n",
      "   -1.61217368e+00 -6.11017883e-01  8.66666660e-02  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.08499795e-02  9.39149976e-01 -3.71033877e-01 -1.45563078e+00\n",
      "   -1.41518605e+00  5.78180075e-01  8.66666660e-02  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.38236859e-01  7.61763155e-01 -1.45653874e-01  4.48718190e-01\n",
      "   -9.34421659e-01 -8.89823437e-01  8.66666660e-02  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.87347718e-02  9.81265247e-01  1.90668795e-02 -1.48105342e-02\n",
      "   -1.76791117e-01 -1.27656567e+00  1.00000001e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.21379428e-02  9.37862098e-01 -1.24038470e+00 -5.32095313e-01\n",
      "   -1.62917411e+00 -6.06486619e-01  1.00000001e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.65547103e-02  9.33445334e-01 -4.35503811e-01 -1.45617259e+00\n",
      "   -1.46073282e+00  5.89254439e-01  1.00000001e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.42183432e-01  7.57816553e-01 -1.65061265e-01  4.35513318e-01\n",
      "   -9.27281380e-01 -9.38449979e-01  1.00000001e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.69822127e-02  9.83017802e-01  2.12074840e-03 -1.95397786e-03\n",
      "   -1.57718807e-01 -1.31675172e+00  1.13333337e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 5.87202348e-02  9.41279769e-01 -1.19491577e+00 -4.99983221e-01\n",
      "   -1.63553905e+00 -5.44179678e-01  1.13333337e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.35708198e-02  9.36429143e-01 -4.42598820e-01 -1.47740078e+00\n",
      "   -1.49348521e+00  5.63420296e-01  1.13333337e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.41738185e-01  7.58261800e-01 -1.31289750e-01  3.71013790e-01\n",
      "   -8.97545874e-01 -1.01038957e+00  1.13333337e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.67717095e-02  9.83228326e-01  3.68349515e-02  6.28923532e-03\n",
      "   -1.32520512e-01 -1.31601274e+00  1.26666665e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 5.94236366e-02  9.40576315e-01 -1.19602823e+00 -5.37195385e-01\n",
      "   -1.62246478e+00 -5.55328250e-01  1.26666665e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.13734908e-02  9.38626468e-01 -4.41684932e-01 -1.49358010e+00\n",
      "   -1.45755279e+00  5.43831706e-01  1.26666665e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.34060287e-01  7.65939653e-01 -9.42312554e-02  4.19957846e-01\n",
      "   -9.26152945e-01 -9.31360483e-01  1.26666665e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.61893032e-02  9.83810782e-01  8.53987783e-02  2.42691785e-02\n",
      "   -1.13662958e-01 -1.30392385e+00  1.40000001e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 5.79469316e-02  9.42053080e-01 -1.24403763e+00 -5.32404542e-01\n",
      "   -1.63993371e+00 -5.59708059e-01  1.40000001e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.31896630e-02  9.36810315e-01 -3.95569652e-01 -1.49429083e+00\n",
      "   -1.43846416e+00  5.21305740e-01  1.40000001e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.33786717e-01  7.66213298e-01 -1.04114614e-01  4.19042468e-01\n",
      "   -8.97868633e-01 -9.46152627e-01  1.40000001e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.81201901e-02  9.81879830e-01  9.18902829e-02  2.65901722e-02\n",
      "   -1.50817379e-01 -1.31161070e+00  1.53333336e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.09979294e-02  9.39002097e-01 -1.24934673e+00 -4.88753587e-01\n",
      "   -1.65283465e+00 -5.61745644e-01  1.53333336e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.22296110e-02  9.37770367e-01 -3.93212289e-01 -1.51863420e+00\n",
      "   -1.42042136e+00  5.72025001e-01  1.53333336e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.32537180e-01  7.67462790e-01 -8.70872140e-02  4.44786549e-01\n",
      "   -8.81740451e-01 -9.26554620e-01  1.53333336e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.79375857e-02  9.82062399e-01  1.07142210e-01 -4.78186458e-03\n",
      "   -1.44037023e-01 -1.30634177e+00  1.66666672e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.01862967e-02  9.39813733e-01 -1.22020543e+00 -5.22258103e-01\n",
      "   -1.67416704e+00 -5.81990778e-01  1.66666672e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]]]\n",
      "$$$:  [[[ 1.77775081e-02  9.82222438e-01 -1.54534757e-01 -3.30899775e-01\n",
      "   -4.83035296e-02 -1.54294872e+00  7.33333305e-02  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 3.99080701e-02  9.60091889e-01 -6.14868760e-01 -5.72747469e-01\n",
      "   -1.50855231e+00 -6.31954610e-01  7.33333305e-02  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 9.56612751e-02  9.04338717e-01  2.66534716e-01 -1.57088614e+00\n",
      "   -2.10070992e+00  2.11124539e-01  7.33333305e-02  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.20739722e-01  7.79260337e-01 -3.34144495e-02  7.95117021e-01\n",
      "   -7.22740233e-01 -8.68142724e-01  7.33333305e-02  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.83213372e-02  9.81678665e-01 -1.43269196e-01 -3.29212666e-01\n",
      "   -4.48220633e-02 -1.55283928e+00  8.66666660e-02  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 3.86100374e-02  9.61389959e-01 -5.94680667e-01 -5.78743219e-01\n",
      "   -1.55287826e+00 -5.94132125e-01  8.66666660e-02  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 9.89340842e-02  9.01065886e-01  2.75775701e-01 -1.56212163e+00\n",
      "   -2.16302013e+00  1.77692249e-01  8.66666660e-02  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.17326149e-01  7.82673836e-01 -7.58128315e-02  8.14614534e-01\n",
      "   -7.25522935e-01 -9.03843462e-01  8.66666660e-02  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.77599825e-02  9.82240021e-01 -1.79930776e-01 -3.45875174e-01\n",
      "   -5.68329655e-02 -1.48679543e+00  1.00000001e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 3.97404321e-02  9.60259616e-01 -5.52096963e-01 -5.65840304e-01\n",
      "   -1.62767184e+00 -6.13561094e-01  1.00000001e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 9.77679193e-02  9.02232051e-01  2.40275547e-01 -1.57735610e+00\n",
      "   -2.18811274e+00  1.95738301e-01  1.00000001e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.27344185e-01  7.72655785e-01 -4.79611643e-02  7.91916549e-01\n",
      "   -7.25297689e-01 -9.09973264e-01  1.00000001e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.81830265e-02  9.81817007e-01 -1.99204713e-01 -2.71107614e-01\n",
      "   -1.12735800e-01 -1.54326272e+00  1.13333337e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 4.04887348e-02  9.59511280e-01 -6.13152146e-01 -5.96778989e-01\n",
      "   -1.65840042e+00 -5.59940815e-01  1.13333337e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 9.09142867e-02  9.09085691e-01  2.11079806e-01 -1.60902619e+00\n",
      "   -2.19505596e+00  1.67958617e-01  1.13333337e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.15871543e-01  7.84128487e-01 -3.27129439e-02  7.20090389e-01\n",
      "   -7.82006264e-01 -8.90745878e-01  1.13333337e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.84412412e-02  9.81558740e-01 -1.52084276e-01 -2.54050434e-01\n",
      "   -7.48397857e-02 -1.52516401e+00  1.26666665e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 4.21048217e-02  9.57895160e-01 -6.02942169e-01 -5.45592308e-01\n",
      "   -1.66208172e+00 -6.13999724e-01  1.26666665e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 8.44232589e-02  9.15576816e-01  1.77335963e-01 -1.66553819e+00\n",
      "   -2.14661169e+00  1.47645548e-01  1.26666665e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.31650665e-01  7.68349349e-01 -6.26221597e-02  7.66780734e-01\n",
      "   -7.61083186e-01 -9.55631196e-01  1.26666665e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.73662715e-02  9.82633770e-01 -1.33196756e-01 -2.23185658e-01\n",
      "   -4.15818021e-02 -1.52196395e+00  1.40000001e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 4.09712307e-02  9.59028721e-01 -6.49072886e-01 -5.26493073e-01\n",
      "   -1.62471640e+00 -6.60888791e-01  1.40000001e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 8.91312808e-02  9.10868704e-01  1.82210535e-01 -1.70270824e+00\n",
      "   -2.11640644e+00  1.93654552e-01  1.40000001e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.23342001e-01  7.76657939e-01 -1.58982724e-02  7.52505779e-01\n",
      "   -7.98226297e-01 -9.12950754e-01  1.40000001e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.67602934e-02  9.83239710e-01 -1.48706183e-01 -2.15952486e-01\n",
      "    2.06813519e-03 -1.51590109e+00  1.53333336e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 4.28599715e-02  9.57140028e-01 -6.81288302e-01 -5.07862031e-01\n",
      "   -1.55680752e+00 -6.99294627e-01  1.53333336e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 8.40896294e-02  9.15910363e-01  2.20154807e-01 -1.65382087e+00\n",
      "   -2.08299136e+00  2.05536351e-01  1.53333336e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.17596307e-01  7.82403708e-01 -2.00031549e-02  7.70256758e-01\n",
      "   -7.73673654e-01 -9.68867779e-01  1.53333336e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.66150965e-02  9.83384967e-01 -1.21835217e-01 -2.36120448e-01\n",
      "    4.40971553e-03 -1.56558800e+00  1.66666672e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 3.83719318e-02  9.61628139e-01 -6.65790081e-01 -5.38364172e-01\n",
      "   -1.60497832e+00 -6.81910276e-01  1.66666672e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]]\n",
      "\n",
      " [[ 1.89123116e-02  9.81087685e-01 -9.87609848e-03 -3.45876776e-02\n",
      "   -1.81206986e-01 -1.24198472e+00  7.33333305e-02  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 5.78275919e-02  9.42172348e-01 -1.28227496e+00 -5.34978092e-01\n",
      "   -1.60203087e+00 -6.28040314e-01  7.33333305e-02  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.17645346e-02  9.38235521e-01 -3.91072243e-01 -1.43466687e+00\n",
      "   -1.34473944e+00  6.11802459e-01  7.33333305e-02  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.40254104e-01  7.59745896e-01 -1.83085486e-01  4.68459427e-01\n",
      "   -9.46856260e-01 -8.50521266e-01  7.33333305e-02  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.90541819e-02  9.80945766e-01 -4.57444275e-03 -3.96661796e-02\n",
      "   -1.97084174e-01 -1.22762072e+00  8.66666660e-02  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 5.81884272e-02  9.41811562e-01 -1.26513696e+00 -5.39531112e-01\n",
      "   -1.61217368e+00 -6.11017883e-01  8.66666660e-02  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.08499795e-02  9.39149976e-01 -3.71033877e-01 -1.45563078e+00\n",
      "   -1.41518605e+00  5.78180075e-01  8.66666660e-02  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.38236859e-01  7.61763155e-01 -1.45653874e-01  4.48718190e-01\n",
      "   -9.34421659e-01 -8.89823437e-01  8.66666660e-02  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.87347718e-02  9.81265247e-01  1.90668795e-02 -1.48105342e-02\n",
      "   -1.76791117e-01 -1.27656567e+00  1.00000001e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.21379428e-02  9.37862098e-01 -1.24038470e+00 -5.32095313e-01\n",
      "   -1.62917411e+00 -6.06486619e-01  1.00000001e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.65547103e-02  9.33445334e-01 -4.35503811e-01 -1.45617259e+00\n",
      "   -1.46073282e+00  5.89254439e-01  1.00000001e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.42183432e-01  7.57816553e-01 -1.65061265e-01  4.35513318e-01\n",
      "   -9.27281380e-01 -9.38449979e-01  1.00000001e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.69822127e-02  9.83017802e-01  2.12074840e-03 -1.95397786e-03\n",
      "   -1.57718807e-01 -1.31675172e+00  1.13333337e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 5.87202348e-02  9.41279769e-01 -1.19491577e+00 -4.99983221e-01\n",
      "   -1.63553905e+00 -5.44179678e-01  1.13333337e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.35708198e-02  9.36429143e-01 -4.42598820e-01 -1.47740078e+00\n",
      "   -1.49348521e+00  5.63420296e-01  1.13333337e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.41738185e-01  7.58261800e-01 -1.31289750e-01  3.71013790e-01\n",
      "   -8.97545874e-01 -1.01038957e+00  1.13333337e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.67717095e-02  9.83228326e-01  3.68349515e-02  6.28923532e-03\n",
      "   -1.32520512e-01 -1.31601274e+00  1.26666665e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 5.94236366e-02  9.40576315e-01 -1.19602823e+00 -5.37195385e-01\n",
      "   -1.62246478e+00 -5.55328250e-01  1.26666665e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.13734908e-02  9.38626468e-01 -4.41684932e-01 -1.49358010e+00\n",
      "   -1.45755279e+00  5.43831706e-01  1.26666665e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.34060287e-01  7.65939653e-01 -9.42312554e-02  4.19957846e-01\n",
      "   -9.26152945e-01 -9.31360483e-01  1.26666665e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.61893032e-02  9.83810782e-01  8.53987783e-02  2.42691785e-02\n",
      "   -1.13662958e-01 -1.30392385e+00  1.40000001e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 5.79469316e-02  9.42053080e-01 -1.24403763e+00 -5.32404542e-01\n",
      "   -1.63993371e+00 -5.59708059e-01  1.40000001e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.31896630e-02  9.36810315e-01 -3.95569652e-01 -1.49429083e+00\n",
      "   -1.43846416e+00  5.21305740e-01  1.40000001e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.33786717e-01  7.66213298e-01 -1.04114614e-01  4.19042468e-01\n",
      "   -8.97868633e-01 -9.46152627e-01  1.40000001e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.81201901e-02  9.81879830e-01  9.18902829e-02  2.65901722e-02\n",
      "   -1.50817379e-01 -1.31161070e+00  1.53333336e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.09979294e-02  9.39002097e-01 -1.24934673e+00 -4.88753587e-01\n",
      "   -1.65283465e+00 -5.61745644e-01  1.53333336e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.22296110e-02  9.37770367e-01 -3.93212289e-01 -1.51863420e+00\n",
      "   -1.42042136e+00  5.72025001e-01  1.53333336e-01  1.33333337e-02\n",
      "    7.07106814e-02  7.07106814e-02  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 2.32537180e-01  7.67462790e-01 -8.70872140e-02  4.44786549e-01\n",
      "   -8.81740451e-01 -9.26554620e-01  1.53333336e-01  1.33333337e-02\n",
      "    3.53553407e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 1.79375857e-02  9.82062399e-01  1.07142210e-01 -4.78186458e-03\n",
      "   -1.44037023e-01 -1.30634177e+00  1.66666672e-01  1.33333337e-02\n",
      "    5.00000007e-02  1.00000001e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]\n",
      "  [ 6.01862967e-02  9.39813733e-01 -1.22020543e+00 -5.22258103e-01\n",
      "   -1.67416704e+00 -5.81990778e-01  1.66666672e-01  1.33333337e-02\n",
      "    7.07106814e-02  1.41421363e-01  1.00000001e-01  1.00000001e-01\n",
      "    2.00000003e-01  2.00000003e-01  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100055. 100055. 100055. 100055. 100055. 100055. 100055. 100055.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-cd7857d25e1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                               \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2080\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2082\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2083\u001b[0m                 \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2084\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-456ecc2a2f21>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outputs/y_pred'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outputs/y_true'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     \"\"\"\n\u001b[0;32m--> 598\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \"\"\"\n\u001b[0;32m--> 570\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_dup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4453\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4454\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4455\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# If you're resuming a previous training, set `initial_epoch` and `final_epoch` accordingly.\n",
    "initial_epoch   = 0\n",
    "final_epoch     = 500\n",
    "steps_per_epoch = 1000\n",
    "\n",
    "# history = model.fit_generator(generator=train_generator,\n",
    "#                               steps_per_epoch=ceil(train_dataset_size/batch_size),\n",
    "#                               epochs=final_epoch,\n",
    "#                               callbacks=callbacks,\n",
    "#                               verbose=1,\n",
    "#                               validation_data=val_generator,\n",
    "#                               validation_steps=ceil(val_dataset_size/batch_size),\n",
    "#                               initial_epoch=initial_epoch)\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=ceil(train_dataset_size/batch_size),\n",
    "                              epochs=final_epoch,\n",
    "                              callbacks=callbacks,\n",
    "                              verbose=1,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=ceil(val_dataset_size/batch_size),\n",
    "                              initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
